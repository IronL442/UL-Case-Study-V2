{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Means Clustering for numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts = pd.read_json('../data/dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts['timestamp'] = pd.to_datetime(df_posts['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract numeric features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of posts over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_per_user = df_posts.groupby('user')['timestamp'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time between consecutive posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the time difference between posts\n",
    "df_posts['time_diff'] = df_posts.groupby('user')['timestamp'].diff()\n",
    "\n",
    "# Convert time_diff to timedelta\n",
    "df_posts['time_diff'] = pd.to_timedelta(df_posts['time_diff'], errors='coerce')\n",
    "\n",
    "# Replace NaT with the median time difference\n",
    "df_posts['time_diff'] = df_posts['time_diff'].fillna(df_posts['time_diff'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time_diff to seconds\n",
    "df_posts['time_diff_seconds'] = df_posts['time_diff'].dt.total_seconds()\n",
    "\n",
    "# Drop the time_diff column\n",
    "df_posts = df_posts.drop(columns=['time_diff'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text-based features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweet length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts['text_length'] = df_posts['text'].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts['word_count'] = df_posts['text'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df = pd.read_csv('../output/sentiment.csv')\n",
    "\n",
    "sentiment_subset = sentiment_df[['text_id', 'sentiment_score']]\n",
    "\n",
    "df_posts = pd.merge(df_posts, sentiment_subset, on='text_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keyword count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hashtag count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts['hashtag_count'] = df_posts['text'].str.count('#')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics from BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = pd.read_csv('../output/posts_with_topics.csv')\n",
    "\n",
    "# Select only 'text_id' and 'topic_label' from topics_df\n",
    "topics_subset = topics_df[['text_id', 'topic_label']]\n",
    "\n",
    "# Merge df_posts with the topics_subset on 'text_id'\n",
    "df_posts = pd.merge(df_posts, topics_subset, on='text_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follower count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "edges = pd.read_csv('../data/graph.csv')\n",
    "\n",
    "# Create a graph\n",
    "G = nx.from_pandas_edgelist(edges, source='source', target='target', edge_attr='weight')\n",
    "\n",
    "# Calculate degrees\n",
    "degrees = dict(G.degree())\n",
    "degree_df = pd.DataFrame(degrees.items(), columns=['user', 'degree'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering Coefficient (how connected a user's neighbors are in the network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_coeff = nx.clustering(G)\n",
    "clustering_df = pd.DataFrame(clustering_coeff.items(), columns=['user', 'clustering_coeff'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Influence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagerank = nx.pagerank(G)\n",
    "pagerank_df = pd.DataFrame(pagerank.items(), columns=['user', 'pagerank'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Betweenness Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m betweenness \u001b[38;5;241m=\u001b[39m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbetweenness_centrality\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m betweenness_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(betweenness\u001b[38;5;241m.\u001b[39mitems(), columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetweenness_centrality\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m<class 'networkx.utils.decorators.argmap'> compilation 33:4\u001b[0m, in \u001b[0;36margmap_betweenness_centrality_29\u001b[0;34m(G, k, normalized, weight, endpoints, seed, backend, **backend_kwargs)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/case-study-ul/venv/lib/python3.11/site-packages/networkx/utils/backends.py:967\u001b[0m, in \u001b[0;36m_dispatchable.__call__\u001b[0;34m(self, backend, *args, **kwargs)\u001b[0m\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m backend \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnetworkx\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    966\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m backend is not installed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 967\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morig_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;66;03m# Use `backend_name` in this function instead of `backend`.\u001b[39;00m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;66;03m# This is purely for aesthetics and to make it easier to search for this\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;66;03m# variable since \"backend\" is used in many comments and log/error messages.\u001b[39;00m\n\u001b[1;32m    972\u001b[0m backend_name \u001b[38;5;241m=\u001b[39m backend\n",
      "File \u001b[0;32m~/Documents/GitHub/case-study-ul/venv/lib/python3.11/site-packages/networkx/algorithms/centrality/betweenness.py:137\u001b[0m, in \u001b[0;36mbetweenness_centrality\u001b[0;34m(G, k, normalized, weight, endpoints, seed)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m nodes:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# single source shortest paths\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# use BFS\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m         S, P, sigma, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_single_source_shortest_path_basic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# use Dijkstra's algorithm\u001b[39;00m\n\u001b[1;32m    139\u001b[0m         S, P, sigma, _ \u001b[38;5;241m=\u001b[39m _single_source_dijkstra_path_basic(G, s, weight)\n",
      "File \u001b[0;32m~/Documents/GitHub/case-study-ul/venv/lib/python3.11/site-packages/networkx/algorithms/centrality/betweenness.py\u001b[0m, in \u001b[0;36m_single_source_shortest_path_basic\u001b[0;34m(G, s)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "betweenness = nx.betweenness_centrality(G)\n",
    "betweenness_df = pd.DataFrame(betweenness.items(), columns=['user', 'betweenness_centrality'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Features per User"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Post Sentiment Per User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts['avg_sentiment'] = df_posts.groupby('user')['sentiment_score'].transform('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variability in Posting Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_std = df_posts.groupby('user')['timestamp'].std().reset_index(name='time_std')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posting Behavior + Text Content: How does sentiment correlate with posting frequency or time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network + Text Analysis: Do influential users post more positive content?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normalize derived numerical features for clustering \n",
    "- Use features for clustering(k-Means or hierarchical)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
