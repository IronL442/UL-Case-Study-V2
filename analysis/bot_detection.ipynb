{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from collections import Counter\n",
    "#from datetime import datetime\n",
    "\n",
    "class BotDetector:\n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        self.df['timestamp'] = pd.to_numeric(self.df['timestamp'])\n",
    "        self.suspicious_users = set()\n",
    "        \n",
    "    def detect_rapid_posting(self, min_posts=3, time_window=10):\n",
    "        \"\"\"Detect users posting many times within short time windows\"\"\"\n",
    "        user_posts = self.df.groupby('user_id').agg({\n",
    "            'timestamp': lambda x: sorted(x.tolist()),\n",
    "            'text_id': 'count'\n",
    "        }).reset_index()\n",
    "        \n",
    "        rapid_posters = []\n",
    "        for _, row in user_posts.iterrows():\n",
    "            if row['text_id'] < min_posts:\n",
    "                continue\n",
    "                \n",
    "            timestamps = row['timestamp']\n",
    "            for i in range(len(timestamps) - min_posts):\n",
    "                window = timestamps[i:i + min_posts]\n",
    "                if window[-1] - window[0] <= time_window:\n",
    "                    rapid_posters.append(row['user_id'])\n",
    "                    break\n",
    "                    \n",
    "        self.suspicious_users.update(rapid_posters)\n",
    "        return rapid_posters\n",
    "    \n",
    "    def detect_duplicate_content(self, similarity_threshold=0.9, min_duplicates=3):\n",
    "        \"\"\"Find users posting very similar content multiple times\"\"\"\n",
    "        from difflib import SequenceMatcher\n",
    "        \n",
    "        def text_similarity(text1, text2):\n",
    "            return SequenceMatcher(None, str(text1), str(text2)).ratio()\n",
    "        \n",
    "        duplicate_posters = []\n",
    "        user_posts = self.df.groupby('user_id')\n",
    "        \n",
    "        for user_id, posts in user_posts:\n",
    "            if len(posts) < min_duplicates:\n",
    "                continue\n",
    "                \n",
    "            texts = posts['text'].tolist()\n",
    "            similar_posts = 0\n",
    "            \n",
    "            for i in range(len(texts)):\n",
    "                for j in range(i + 1, len(texts)):\n",
    "                    if text_similarity(texts[i], texts[j]) >= similarity_threshold:\n",
    "                        similar_posts += 1\n",
    "                        if similar_posts >= min_duplicates:\n",
    "                            duplicate_posters.append(user_id)\n",
    "                            break\n",
    "                if user_id in duplicate_posters:\n",
    "                    break\n",
    "                    \n",
    "        self.suspicious_users.update(duplicate_posters)\n",
    "        return duplicate_posters\n",
    "    \n",
    "    def detect_periodic_posting(self, min_posts=10, variance_threshold=0.1):\n",
    "        \"\"\"Identify users posting at suspiciously regular intervals\"\"\"\n",
    "        periodic_posters = []\n",
    "        user_posts = self.df.groupby('user_id')\n",
    "        \n",
    "        for user_id, posts in user_posts:\n",
    "            if len(posts) < min_posts:\n",
    "                continue\n",
    "                \n",
    "            timestamps = sorted(posts['timestamp'])\n",
    "            intervals = np.diff(timestamps)\n",
    "            \n",
    "            # Calculate coefficient of variation (normalized variance)\n",
    "            cv = np.std(intervals) / np.mean(intervals)\n",
    "            if cv < variance_threshold:\n",
    "                periodic_posters.append(user_id)\n",
    "                \n",
    "        self.suspicious_users.update(periodic_posters)\n",
    "        return periodic_posters\n",
    "    \n",
    "    def get_bot_likelihood_scores(self):\n",
    "        \"\"\"Calculate a bot likelihood score for each user\"\"\"\n",
    "        scores = {}\n",
    "        total_checks = 3  # Number of detection methods\n",
    "        \n",
    "        rapid = set(self.detect_rapid_posting())\n",
    "        duplicates = set(self.detect_duplicate_content())\n",
    "        periodic = set(self.detect_periodic_posting())\n",
    "        \n",
    "        all_users = set(self.df['user_id'].unique())\n",
    "        \n",
    "        for user in all_users:\n",
    "            score = 0\n",
    "            if user in rapid: score += 1\n",
    "            if user in duplicates: score += 1\n",
    "            if user in periodic: score += 1\n",
    "            scores[user] = score / total_checks\n",
    "            \n",
    "        return pd.DataFrame([\n",
    "            {'user_id': user, 'bot_likelihood': score} \n",
    "            for user, score in scores.items()\n",
    "        ]).sort_values('bot_likelihood', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1005036246, 1007621559, 1007690223, 1010626427, 1013703855, 1018419764, 1031499948, 1045532631, 1049151527, 1050902549, 1050929533, 1055454345, 1056254952, 1061458474, 1061862662, 1064632123, 1064791428, 1068051022, 1072645177, 1077881120, 1081539622, 1099719517]\n"
     ]
    }
   ],
   "source": [
    "# First load your JSON dataset\n",
    "from pandas import json_normalize\n",
    "import json\n",
    "\n",
    "df = pd.read_json('../data/dataset.json')\n",
    "\n",
    "# Create detector instance and analyze\n",
    "detector = BotDetector(df)\n",
    "suspicious_users = detector.get_bot_likelihood_scores()\n",
    "\n",
    "# View top suspicious users\n",
    "#print(suspicious_users.head(10))\n",
    "\n",
    "# For specific behaviors, run individual methods:\n",
    "rapid_posters = detector.detect_rapid_posting()\n",
    "duplicate_posters = detector.detect_duplicate_content()\n",
    "periodic_posters = detector.detect_periodic_posting()\n",
    "\n",
    "print(duplicate_posters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
