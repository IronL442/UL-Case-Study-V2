{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic classification with BERTopic\n",
    "\n",
    "Install transformers 4.41.0 for compatability with spacy and BERTopic\n",
    "\n",
    "`pip install transformers==4.41.0` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Careful when running this, very hardware intensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauritseisengarten/Documents/GitHub/case-study-ul/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import psutil\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>text_id</th>\n",
       "      <th>user</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>Running a business means juggling countless ad...</td>\n",
       "      <td>2018569761</td>\n",
       "      <td>danielwoodard</td>\n",
       "      <td>1077866112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>Liz Truss is walking in the lingering shadow o...</td>\n",
       "      <td>2092717718</td>\n",
       "      <td>nelsonjacqueline</td>\n",
       "      <td>1089670430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>The UK is bracing for war as government buildi...</td>\n",
       "      <td>2059143248</td>\n",
       "      <td>ihooper</td>\n",
       "      <td>1007478642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>Marrying a second or third cousin once removed...</td>\n",
       "      <td>2008209828</td>\n",
       "      <td>wrightnicholas</td>\n",
       "      <td>1039258480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>It's truly disgraceful how the Indian National...</td>\n",
       "      <td>2001239278</td>\n",
       "      <td>michael51</td>\n",
       "      <td>1021455936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp                                               text     text_id  \\\n",
       "0 2024-10-31  Running a business means juggling countless ad...  2018569761   \n",
       "1 2024-10-31  Liz Truss is walking in the lingering shadow o...  2092717718   \n",
       "2 2024-10-31  The UK is bracing for war as government buildi...  2059143248   \n",
       "3 2024-10-31  Marrying a second or third cousin once removed...  2008209828   \n",
       "4 2024-10-31  It's truly disgraceful how the Indian National...  2001239278   \n",
       "\n",
       "               user     user_id  \n",
       "0     danielwoodard  1077866112  \n",
       "1  nelsonjacqueline  1089670430  \n",
       "2           ihooper  1007478642  \n",
       "3    wrightnicholas  1039258480  \n",
       "4         michael51  1021455936  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_posts = pd.read_json('../data/dataset.json')\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply minimal preprocessing (Remove URLs and Mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts['text'] = df_posts['text'].apply(lambda x: re.sub(r'http\\S+|www\\S+|https\\S+|@\\w+', '', x) if pd.notna(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_posts = df_posts.sample(frac=0.1, random_state=42)  # 10% of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor memory usage\n",
    "def monitor_memory():\n",
    "    process = psutil.Process()\n",
    "    memory_gb = process.memory_info().rss / 1024 / 1024 / 1024\n",
    "    return f\"Memory Usage: {memory_gb:.2f} GB\"\n",
    "\n",
    "# Create embeddings with memory monitoring and larger batches\n",
    "def create_multifeature_embeddings(df_posts, sentence_model, batch_size=64):\n",
    "    print(f\"\\nStarting embedding generation for {len(df_posts)} documents\")\n",
    "    print(monitor_memory())\n",
    "\n",
    "    print(\"\\nGenerating text embeddings...\")\n",
    "    text_embeddings = sentence_model.encode(\n",
    "        df_posts['text'].fillna(\"\").tolist(),\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    print(monitor_memory())\n",
    "\n",
    "    print(monitor_memory())\n",
    "    return text_embeddings\n",
    "\n",
    "# Set up and train BERTopic model with memory optimization\n",
    "def setup_bertopic_model(df_posts, batch_size=64):\n",
    "    print(f\"\\nDataset size: {len(df_posts)} documents\")\n",
    "    print(f\"DataFrame memory usage: {df_posts.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "    print(monitor_memory())\n",
    "\n",
    "    print(\"\\nInitializing models...\")\n",
    "    sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    vectorizer_model = CountVectorizer(\n",
    "        stop_words=\"english\",\n",
    "        #min_df = 350,  # Terms must appear in at least 350 rows (0.5% of dataset)\n",
    "        #max_df = 0.8,  # Terms must appear in less than 80% of the dataset (56,000 rows)\n",
    "        ngram_range=(1, 3) # Increase to include trigrams\n",
    "    )\n",
    "\n",
    "    umap_model = UMAP(\n",
    "        n_neighbors=15,\n",
    "        n_components=5,\n",
    "        min_dist=0.0,\n",
    "        metric='cosine',\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    embeddings = create_multifeature_embeddings(df_posts, sentence_model, batch_size)\n",
    "\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=sentence_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        umap_model=umap_model,\n",
    "        #min_topic_size = 20,      # uncomment to set manual minimum topic size\n",
    "        nr_topics='auto',\n",
    "        calculate_probabilities=True,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    print(\"\\nFitting BERTopic model...\")\n",
    "    topics, probs = topic_model.fit_transform(\n",
    "        documents=df_posts['text'].fillna(\"\").tolist(),\n",
    "        embeddings=embeddings\n",
    "    )\n",
    "\n",
    "    return topic_model, topics, probs\n",
    "\n",
    "# Analyze topics with memory considerations\n",
    "def analyze_topics(topic_model, topics, df_posts):\n",
    "    print(\"\\nAnalyzing topics...\")\n",
    "    print(monitor_memory())\n",
    "\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "\n",
    "    df_posts['topic'] = topics\n",
    "\n",
    "    topic_docs = {}\n",
    "    unique_topics = set(topics)\n",
    "    print(f\"\\nFound {len(unique_topics)-1} topics (excluding -1)\")\n",
    "\n",
    "    for topic in tqdm(unique_topics):\n",
    "        if topic != -1:\n",
    "            topic_docs[topic] = df_posts[df_posts['topic'] == topic]['text'].head(3).tolist()\n",
    "\n",
    "    try:\n",
    "        print(\"\\nGenerating visualizations...\")\n",
    "        topic_model.visualize_topics()\n",
    "        topic_model.visualize_hierarchy()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Visualization error: {e}\")\n",
    "\n",
    "    return topic_info, topic_docs\n",
    "\n",
    "# Run the complete pipeline with memory monitoring\n",
    "def run_topic_analysis(df_posts, batch_size=64):\n",
    "    print(f\"Starting analysis with batch size: {batch_size}\")\n",
    "    print(monitor_memory())\n",
    "\n",
    "    if 'text' not in df_posts.columns:\n",
    "        raise ValueError(\"Missing required column: text\")\n",
    "\n",
    "    topic_model, topics, probs = setup_bertopic_model(df_posts, batch_size)\n",
    "    topic_info, topic_docs = analyze_topics(topic_model, topics, df_posts)\n",
    "\n",
    "    summary = {\n",
    "        'num_topics': len(set(topics)) - 1,\n",
    "        'topic_sizes': topic_info['Count'].tolist(),\n",
    "        'top_topics': topic_info.head(10).to_dict('records')\n",
    "    }\n",
    "\n",
    "    return topic_model, summary, topics, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis with batch size: 64\n",
      "Memory Usage: 0.69 GB\n",
      "\n",
      "Dataset size: 70260 documents\n",
      "DataFrame memory usage: 2.68 MB\n",
      "Memory Usage: 0.69 GB\n",
      "\n",
      "Initializing models...\n",
      "\n",
      "Starting embedding generation for 70260 documents\n",
      "Memory Usage: 0.79 GB\n",
      "\n",
      "Generating text embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1098/1098 [01:49<00:00, 10.05it/s]\n",
      "2025-01-26 16:00:19,697 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage: 1.99 GB\n",
      "Memory Usage: 1.99 GB\n",
      "\n",
      "Fitting BERTopic model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n",
      "2025-01-26 16:01:07,388 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-01-26 16:01:07,389 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "try:\n",
    "    topic_model, summary, topics, probs = run_topic_analysis(df_posts, batch_size=64)\n",
    "    \n",
    "    print(f\"\\nAnalysis complete!\")\n",
    "    print(f\"Found {summary['num_topics']} topics\")\n",
    "    print(\"\\nTop 10 topics:\")\n",
    "    for topic in summary['top_topics']:\n",
    "        print(f\"Topic {topic['Topic']}: Size {topic['Count']}\")\n",
    "    \n",
    "    topic_model.save(\"bertopic_model_large\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during analysis: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most frequent topics with their terms:\n",
      "\n",
      "Topic 0 (Size: 3739):\n",
      "  - health: 0.013\n",
      "  - business: 0.011\n",
      "  - let: 0.009\n",
      "  - vote: 0.007\n",
      "  - time: 0.007\n",
      "  - check: 0.007\n",
      "  - just: 0.007\n",
      "  - support: 0.007\n",
      "  - climate: 0.007\n",
      "  - politics: 0.006\n",
      "\n",
      "Topic 1 (Size: 152):\n",
      "  - tuchel: 0.044\n",
      "  - chelsea: 0.041\n",
      "  - football: 0.041\n",
      "  - business: 0.030\n",
      "  - club: 0.025\n",
      "  - just: 0.020\n",
      "  - arsenal: 0.019\n",
      "  - game: 0.018\n",
      "  - chelseafc: 0.018\n",
      "  - players: 0.018\n",
      "\n",
      "Topic 2 (Size: 68):\n",
      "  - unknown: 0.102\n",
      "  - climateemergency scotradar cop26: 0.080\n",
      "  - climateemergency scotradar: 0.080\n",
      "  - times climateemergency: 0.080\n",
      "  - times climateemergency scotradar: 0.080\n",
      "  - scotradar cop26: 0.080\n",
      "  - scotradar: 0.080\n",
      "  - alt: 0.080\n",
      "  - scotradar cop26 cop26glasgow: 0.079\n",
      "  - cop26 cop26glasgow: 0.078\n",
      "\n",
      "Topic 3 (Size: 64):\n",
      "  - health: 0.084\n",
      "  - weightloss: 0.063\n",
      "  - fat: 0.060\n",
      "  - weight: 0.051\n",
      "  - diet: 0.044\n",
      "  - fitness: 0.043\n",
      "  - wellness: 0.033\n",
      "  - tips: 0.032\n",
      "  - health weightloss: 0.030\n",
      "  - loss: 0.029\n",
      "\n",
      "Topic 4 (Size: 36):\n",
      "  - assignments: 0.087\n",
      "  - help: 0.074\n",
      "  - essays: 0.054\n",
      "  - assistance: 0.053\n",
      "  - exams: 0.051\n",
      "  - struggling: 0.049\n",
      "  - dm: 0.045\n",
      "  - subjects: 0.043\n",
      "  - ve got covered: 0.038\n",
      "  - ve got: 0.038\n",
      "\n",
      "Topic 5 (Size: 22):\n",
      "  - cart: 0.078\n",
      "  - usa variant: 0.078\n",
      "  - mint 699: 0.078\n",
      "  - cart mint 699: 0.078\n",
      "  - 699: 0.078\n",
      "  - nintendo 64 n64: 0.078\n",
      "  - mint 699 99: 0.078\n",
      "  - mercy usa variant: 0.078\n",
      "  - nintendo 64: 0.078\n",
      "  - nintendo: 0.078\n",
      "\n",
      "Topic 6 (Size: 14):\n",
      "  - cybersecurity: 0.099\n",
      "  - ransomware: 0.097\n",
      "  - security: 0.062\n",
      "  - attacks: 0.053\n",
      "  - threats: 0.042\n",
      "  - ransomware attacks: 0.037\n",
      "  - cyber: 0.036\n",
      "  - protect: 0.033\n",
      "  - learn: 0.033\n",
      "  - digital: 0.032\n",
      "\n",
      "Topic 7 (Size: 14):\n",
      "  - pollution: 0.201\n",
      "  - near: 0.198\n",
      "  - pollution low: 0.174\n",
      "  - low: 0.113\n",
      "  - road pollution: 0.111\n",
      "  - road pollution low: 0.111\n",
      "  - road: 0.094\n",
      "  - lambeth: 0.060\n",
      "  - damp: 0.060\n",
      "  - near lambeth: 0.060\n",
      "\n",
      "Topic 8 (Size: 11):\n",
      "  - empire: 0.240\n",
      "  - empire isn: 0.168\n",
      "  - isn: 0.120\n",
      "  - loss: 0.115\n",
      "  - loss best: 0.109\n",
      "  - best: 0.074\n",
      "  - business: 0.054\n",
      "  - business empire empire: 0.049\n",
      "  - نون loss: 0.049\n",
      "  - empire isn نون: 0.049\n",
      "\n",
      "Topic 9 (Size: 11):\n",
      "  - content analysis: 0.198\n",
      "  - bot: 0.193\n",
      "  - 100: 0.186\n",
      "  - content: 0.172\n",
      "  - analysis: 0.168\n",
      "  - complete report: 0.145\n",
      "  - report: 0.144\n",
      "  - complete: 0.131\n",
      "  - 100 complete: 0.126\n",
      "  - analysis article: 0.126\n",
      "\n",
      "Topic 10 (Size: 11):\n",
      "  - climate: 0.149\n",
      "  - fudge cake: 0.130\n",
      "  - anniversary declaring: 0.130\n",
      "  - climate fudge: 0.130\n",
      "  - october 2nd: 0.130\n",
      "  - 2nd anniversary declaring: 0.130\n",
      "  - 2nd anniversary: 0.130\n",
      "  - meter tall climate: 0.130\n",
      "  - 12 october 2nd: 0.130\n",
      "  - 12 october: 0.130\n"
     ]
    }
   ],
   "source": [
    "# 1. Save visualizations to HTML files\n",
    "fig = topic_model.visualize_barchart(top_n_topics=20)\n",
    "fig.write_html(\"../output/topic_barchart.html\")\n",
    "\n",
    "topic_model.visualize_topics().write_html(\"../output/topic_clusters.html\")\n",
    "topic_model.visualize_hierarchy().write_html(\"../output/topic_hierarchy.html\")\n",
    "\n",
    "# 2. Print text-based summary\n",
    "topics_info = topic_model.get_topic_info()\n",
    "print(\"\\nMost frequent topics with their terms:\")\n",
    "for _, row in topics_info.head(20).iterrows():\n",
    "    topic_id = row['Topic']\n",
    "    size = row['Count']\n",
    "    if topic_id != -1:\n",
    "        terms = topic_model.get_topic(topic_id)\n",
    "        print(f\"\\nTopic {topic_id} (Size: {size}):\")\n",
    "        # Print top 10 terms for each topic with their weights\n",
    "        for term, weight in terms[:20]:\n",
    "            print(f\"  - {term}: {weight:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform documents into topics and probabilities\n",
    "#documents = df_posts['text'].tolist()\n",
    "#topics, probs = topic_model.transform(documents)\n",
    "\n",
    "# Reassign \"Other\" topics to the most probable topic\n",
    "#import numpy as np\n",
    "#for i, prob in enumerate(probs):\n",
    "#    if topics[i] == -1:  # Check for \"Other\"\n",
    "#        topics[i] = np.argmax(prob)  # Assign the most probable topic\n",
    "\n",
    "# Add topic numbers to the dataframe\n",
    "#df_posts['topic'] = topics\n",
    "\n",
    "# Generate topic labels\n",
    "#topic_labels = {}\n",
    "#for topic_id in set(topics):\n",
    "#    if topic_id != -1:\n",
    "#        terms = topic_model.get_topic(topic_id)\n",
    "#        topic_labels[topic_id] = \", \".join([term for term, _ in terms[:3]])\n",
    "#    else:\n",
    "#        topic_labels[topic_id] = \"Other\"\n",
    "\n",
    "# Add descriptive topic labels to the dataframe\n",
    "#df_posts['topic_label'] = df_posts['topic'].map(topic_labels)\n",
    "\n",
    "# Save augmented dataframe\n",
    "#df_posts.to_csv('../output/posts_with_topics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 220/220 [00:10<00:00, 21.19it/s]\n",
      "2025-01-26 15:57:27,948 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-01-26 15:57:29,462 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-01-26 15:57:29,462 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-01-26 15:57:29,568 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
      "2025-01-26 15:57:31,404 - BERTopic - Probabilities - Completed ✓\n",
      "2025-01-26 15:57:31,405 - BERTopic - Cluster - Completed ✓\n",
      "2025-01-26 15:57:31,413 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-01-26 15:57:31,413 - BERTopic - Topic reduction - Reduced number of topics from 12 to 12\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 11 is out of bounds for axis 1 with size 11",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m         topics[i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(prob)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Reduce topics to a smaller number for feature clustering\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtopic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnr_topics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Generate labels for the topics\u001b[39;00m\n\u001b[1;32m     15\u001b[0m topic_labels \u001b[38;5;241m=\u001b[39m {topic_id: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([term \u001b[38;5;28;01mfor\u001b[39;00m term, _ \u001b[38;5;129;01min\u001b[39;00m topic_model\u001b[38;5;241m.\u001b[39mget_topic(topic_id)[:\u001b[38;5;241m3\u001b[39m]]) \n\u001b[1;32m     16\u001b[0m                \u001b[38;5;28;01mfor\u001b[39;00m topic_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(topics) \u001b[38;5;28;01mif\u001b[39;00m topic_id \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m}\n",
      "File \u001b[0;32m~/Documents/GitHub/case-study-ul/venv/lib/python3.11/site-packages/bertopic/_bertopic.py:2216\u001b[0m, in \u001b[0;36mBERTopic.reduce_topics\u001b[0;34m(self, docs, nr_topics, images, use_ctfidf)\u001b[0m\n\u001b[1;32m   2214\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merged_topics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_representative_docs(documents)\n\u001b[0;32m-> 2216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobabilities_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_probabilities\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobabilities_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Documents/GitHub/case-study-ul/venv/lib/python3.11/site-packages/bertopic/_bertopic.py:4547\u001b[0m, in \u001b[0;36mBERTopic._map_probabilities\u001b[0;34m(self, probabilities, original_topics)\u001b[0m\n\u001b[1;32m   4545\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m from_topic, to_topic \u001b[38;5;129;01min\u001b[39;00m mappings\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4546\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m to_topic \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m from_topic \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 4547\u001b[0m                 mapped_probabilities[:, to_topic] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mprobabilities\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_topic\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   4549\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mapped_probabilities\n\u001b[1;32m   4551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m probabilities\n",
      "\u001b[0;31mIndexError\u001b[0m: index 11 is out of bounds for axis 1 with size 11"
     ]
    }
   ],
   "source": [
    "# \n",
    "documents = df_posts['text'].tolist()\n",
    "topics, probs = topic_model.transform(documents)\n",
    "\n",
    "# Handle \"Other\" first\n",
    "num_others = np.sum(np.array(topics) == -1)\n",
    "for i, prob in enumerate(probs):\n",
    "    if topics[i] == -1:\n",
    "        topics[i] = np.argmax(prob)\n",
    "\n",
    "# Reduce topics to a smaller number for feature clustering\n",
    "topic_model.reduce_topics(documents, nr_topics=30)\n",
    "\n",
    "# Generate labels for the topics\n",
    "topic_labels = {topic_id: \", \".join([term for term, _ in topic_model.get_topic(topic_id)[:3]]) \n",
    "               for topic_id in set(topics) if topic_id != -1}\n",
    "topic_labels[-1] = \"Other\"\n",
    "\n",
    "df_posts['topic'] = topics\n",
    "df_posts['topic_label'] = df_posts['topic'].map(topic_labels)\n",
    "df_posts.to_csv('../output/posts_with_topics.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
