{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic classification with BERTopic\n",
    "\n",
    "Install transformers 4.41.0 for compatability with spacy and BERTopic\n",
    "\n",
    "`pip install transformers==4.41.0` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Careful when running this, very hardware intensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import psutil\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts = pd.read_json('../data/dataset.json')\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply minimal preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_pattern = re.compile(r'http\\S+|www\\S+|https\\S+')  # Removes URLs\n",
    "\n",
    "df_posts['text'] = df_posts['text'].apply(lambda x: url_pattern.sub('', x) if pd.notna(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts = df_posts.sample(n=10000, random_state=42)  # Uncomment to test with sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_memory():\n",
    "    \"\"\"Monitor memory usage\"\"\"\n",
    "    process = psutil.Process()\n",
    "    memory_gb = process.memory_info().rss / 1024 / 1024 / 1024\n",
    "    return f\"Memory Usage: {memory_gb:.2f} GB\"\n",
    "\n",
    "def create_multifeature_embeddings(df_posts, sentence_model, batch_size=64):\n",
    "    \"\"\"\n",
    "    Create combined embeddings with memory monitoring and larger batches\n",
    "    \"\"\"\n",
    "    print(f\"\\nStarting embedding generation for {len(df_posts)} documents\")\n",
    "    print(monitor_memory())\n",
    "    \n",
    "    def safe_join(items):\n",
    "        if not items or (isinstance(items, list) and len(items) == 0):\n",
    "            return \"\"\n",
    "        return \" \".join(str(item) for item in items)\n",
    "    \n",
    "    print(\"\\nGenerating text embeddings...\")\n",
    "    text_embeddings = sentence_model.encode(\n",
    "        df_posts['text'].fillna(\"\").tolist(),\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    print(monitor_memory())\n",
    "    \n",
    "    print(\"\\nGenerating hashtag embeddings...\")\n",
    "    hashtag_embeddings = sentence_model.encode(\n",
    "        [safe_join(tags) for tags in df_posts['hashtags']],\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    print(monitor_memory())\n",
    "    \n",
    "    \n",
    "    print(monitor_memory())\n",
    "    \n",
    "    print(\"\\nCombining embeddings...\")\n",
    "    combined_embeddings = (\n",
    "        0.95 * text_embeddings + \n",
    "        0.05 * hashtag_embeddings\n",
    "    )\n",
    "    \n",
    "    # Clean up to free memory\n",
    "    del text_embeddings, hashtag_embeddings\n",
    "    gc.collect()\n",
    "    \n",
    "    print(monitor_memory())\n",
    "    return combined_embeddings\n",
    "\n",
    "def setup_bertopic_model(df_posts, batch_size=64):\n",
    "    \"\"\"\n",
    "    Set up and train BERTopic model with memory optimization\n",
    "    \"\"\"\n",
    "    print(f\"\\nDataset size: {len(df_posts)} documents\")\n",
    "    print(f\"DataFrame memory usage: {df_posts.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "    print(monitor_memory())\n",
    "    \n",
    "    print(\"\\nInitializing models...\")\n",
    "    sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Modified parameters for large dataset\n",
    "    vectorizer_model = CountVectorizer(\n",
    "        stop_words=\"english\",\n",
    "        #min_df=2,    # Term must appear in at least 3 documents\n",
    "        #max_df=0.7,  # Ignore terms that appear in >50% of documents\n",
    "        ngram_range=(1, 2)\n",
    "    )\n",
    "    \n",
    "    # Optimized UMAP settings for large dataset\n",
    "    umap_model = UMAP(\n",
    "        n_neighbors=15,\n",
    "        n_components=5,\n",
    "        min_dist=0.0,\n",
    "        metric='cosine',\n",
    "        low_memory=True,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create embeddings\n",
    "    embeddings = create_multifeature_embeddings(df_posts, sentence_model, batch_size)\n",
    "    \n",
    "    # Initialize BERTopic with optimized settings\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=sentence_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        umap_model=umap_model,\n",
    "        min_topic_size=30,  # Increased for larger dataset\n",
    "        nr_topics='auto',\n",
    "        calculate_probabilities=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\nFitting BERTopic model...\")\n",
    "    topics, probs = topic_model.fit_transform(\n",
    "        documents=df_posts['text'].fillna(\"\").tolist(),\n",
    "        embeddings=embeddings\n",
    "    )\n",
    "    \n",
    "    return topic_model, topics, probs\n",
    "\n",
    "def analyze_topics(topic_model, topics, df_posts):\n",
    "    \"\"\"\n",
    "    Analyze topics with memory considerations\n",
    "    \"\"\"\n",
    "    print(\"\\nAnalyzing topics...\")\n",
    "    print(monitor_memory())\n",
    "    \n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    \n",
    "    # More memory-efficient way to store topics\n",
    "    df_posts['topic'] = topics\n",
    "    \n",
    "    # Get sample documents for each topic (limited to save memory)\n",
    "    topic_docs = {}\n",
    "    unique_topics = set(topics)\n",
    "    print(f\"\\nFound {len(unique_topics)-1} topics (excluding -1)\")\n",
    "    \n",
    "    for topic in tqdm(unique_topics):\n",
    "        if topic != -1:\n",
    "            topic_docs[topic] = df_posts[df_posts['topic'] == topic]['text'].head(3).tolist()\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nGenerating visualizations...\")\n",
    "        topic_model.visualize_topics()\n",
    "        topic_model.visualize_hierarchy()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Visualization error: {e}\")\n",
    "    \n",
    "    return topic_info, topic_docs\n",
    "\n",
    "def run_topic_analysis(df_posts, batch_size=64):\n",
    "    \"\"\"\n",
    "    Run the complete pipeline with memory monitoring\n",
    "    \"\"\"\n",
    "    print(f\"Starting analysis with batch size: {batch_size}\")\n",
    "    print(monitor_memory())\n",
    "    \n",
    "    required_columns = ['text', 'hashtags']\n",
    "    if not all(col in df_posts.columns for col in required_columns):\n",
    "        raise ValueError(f\"Missing columns. Required: {required_columns}\")\n",
    "    \n",
    "    # Optionally sample for testing\n",
    "    #df_posts = df_posts.sample(n=10000, random_state=42)  # Uncomment to test with sample\n",
    "    \n",
    "    topic_model, topics, probs = setup_bertopic_model(df_posts, batch_size)\n",
    "    topic_info, topic_docs = analyze_topics(topic_model, topics, df_posts)\n",
    "    \n",
    "    # Create memory-efficient summary\n",
    "    summary = {\n",
    "        'num_topics': len(set(topics)) - 1,\n",
    "        'topic_sizes': topic_info['Count'].tolist(),\n",
    "        'top_topics': topic_info.head(10).to_dict('records')\n",
    "    }\n",
    "    \n",
    "    return topic_model, summary, topics, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis with batch size: 64\n",
      "Memory Usage: 0.31 GB\n",
      "\n",
      "Dataset size: 10000 documents\n",
      "DataFrame memory usage: 0.84 MB\n",
      "Memory Usage: 0.32 GB\n",
      "\n",
      "Initializing models...\n",
      "\n",
      "Starting embedding generation for 10000 documents\n",
      "Memory Usage: 0.41 GB\n",
      "\n",
      "Generating text embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 157/157 [00:10<00:00, 15.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage: 0.85 GB\n",
      "\n",
      "Generating hashtag embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 157/157 [00:09<00:00, 16.91it/s]\n",
      "2025-01-23 12:25:00,137 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage: 1.20 GB\n",
      "Memory Usage: 1.20 GB\n",
      "\n",
      "Combining embeddings...\n",
      "Memory Usage: 1.24 GB\n",
      "\n",
      "Fitting BERTopic model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-23 12:25:09,952 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-01-23 12:25:09,952 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-01-23 12:25:10,591 - BERTopic - Cluster - Completed ✓\n",
      "2025-01-23 12:25:10,591 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2025-01-23 12:25:10,914 - BERTopic - Representation - Completed ✓\n",
      "2025-01-23 12:25:10,915 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-01-23 12:25:11,231 - BERTopic - Topic reduction - Reduced number of topics from 53 to 28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing topics...\n",
      "Memory Usage: 1.35 GB\n",
      "\n",
      "Found 27 topics (excluding -1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:00<00:00, 7755.94it/s]\n",
      "2025-01-23 12:25:11,457 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating visualizations...\n",
      "\n",
      "Analysis complete!\n",
      "Found 27 topics\n",
      "\n",
      "Top 10 topics:\n",
      "Topic -1: Size 4502\n",
      "Topic 0: Size 1011\n",
      "Topic 1: Size 914\n",
      "Topic 2: Size 849\n",
      "Topic 3: Size 616\n",
      "Topic 4: Size 308\n",
      "Topic 5: Size 277\n",
      "Topic 6: Size 217\n",
      "Topic 7: Size 184\n",
      "Topic 8: Size 114\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    topic_model, summary, topics, probs = run_topic_analysis(df_posts, batch_size=64)\n",
    "    \n",
    "    print(f\"\\nAnalysis complete!\")\n",
    "    print(f\"Found {summary['num_topics']} topics\")\n",
    "    print(\"\\nTop 10 topics:\")\n",
    "    for topic in summary['top_topics']:\n",
    "        print(f\"Topic {topic['Topic']}: Size {topic['Count']}\")\n",
    "    \n",
    "    topic_model.save(\"bertopic_model_large\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during analysis: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most frequent topics with their terms:\n",
      "\n",
      "Topic 0 (Size: 1011):\n",
      "  - vote: 0.042\n",
      "  - artist: 0.037\n",
      "  - voting: 0.030\n",
      "  - social artist: 0.029\n",
      "  - let: 0.027\n",
      "  - social: 0.027\n",
      "  - btsbbmas: 0.026\n",
      "  - bbmas: 0.024\n",
      "  - army: 0.023\n",
      "  - amas: 0.022\n",
      "\n",
      "Topic 1 (Size: 914):\n",
      "  - politics: 0.041\n",
      "  - political: 0.017\n",
      "  - time: 0.012\n",
      "  - trump: 0.011\n",
      "  - people: 0.010\n",
      "  - like: 0.009\n",
      "  - truss: 0.009\n",
      "  - corruption: 0.009\n",
      "  - truth: 0.008\n",
      "  - just: 0.008\n",
      "\n",
      "Topic 2 (Size: 849):\n",
      "  - health: 0.050\n",
      "  - mental: 0.038\n",
      "  - mental health: 0.036\n",
      "  - healthcare: 0.023\n",
      "  - mentalhealth: 0.019\n",
      "  - covid: 0.016\n",
      "  - mentalhealthawareness: 0.015\n",
      "  - care: 0.013\n",
      "  - secretary: 0.013\n",
      "  - medical: 0.012\n",
      "\n",
      "Topic 3 (Size: 616):\n",
      "  - business: 0.048\n",
      "  - marketing: 0.019\n",
      "  - entrepreneurship: 0.018\n",
      "  - check: 0.015\n",
      "  - success: 0.014\n",
      "  - online: 0.013\n",
      "  - boost: 0.012\n",
      "  - sales: 0.011\n",
      "  - looking: 0.011\n",
      "  - social media: 0.010\n",
      "\n",
      "Topic 4 (Size: 308):\n",
      "  - science: 0.094\n",
      "  - research: 0.017\n",
      "  - scientific: 0.015\n",
      "  - knowledge: 0.012\n",
      "  - world: 0.012\n",
      "  - innovation: 0.011\n",
      "  - sciencematters: 0.010\n",
      "  - check: 0.010\n",
      "  - book: 0.009\n",
      "  - latest: 0.009\n",
      "\n",
      "Topic 5 (Size: 277):\n",
      "  - tuchel: 0.039\n",
      "  - football: 0.035\n",
      "  - chelsea: 0.032\n",
      "  - game: 0.023\n",
      "  - business: 0.021\n",
      "  - players: 0.019\n",
      "  - club: 0.018\n",
      "  - sports: 0.017\n",
      "  - politics: 0.017\n",
      "  - team: 0.016\n",
      "\n",
      "Topic 6 (Size: 217):\n",
      "  - health: 0.064\n",
      "  - wellness: 0.041\n",
      "  - diet: 0.029\n",
      "  - fitness: 0.029\n",
      "  - body: 0.024\n",
      "  - healthyliving: 0.024\n",
      "  - fat: 0.023\n",
      "  - discover: 0.022\n",
      "  - health wellness: 0.021\n",
      "  - weightloss: 0.021\n",
      "\n",
      "Topic 7 (Size: 184):\n",
      "  - climate: 0.076\n",
      "  - climate change: 0.050\n",
      "  - change: 0.044\n",
      "  - climatechange: 0.037\n",
      "  - climatecrisis: 0.026\n",
      "  - energy: 0.025\n",
      "  - climateaction: 0.023\n",
      "  - environmental: 0.017\n",
      "  - science: 0.016\n",
      "  - action: 0.016\n",
      "\n",
      "Topic 8 (Size: 114):\n",
      "  - oct climate: 0.263\n",
      "  - climate high: 0.263\n",
      "  - high low: 0.263\n",
      "  - precip: 0.263\n",
      "  - low precip: 0.263\n",
      "  - snow: 0.263\n",
      "  - oct: 0.259\n",
      "  - low: 0.246\n",
      "  - high: 0.237\n",
      "  - precip snow: 0.227\n",
      "\n",
      "Topic 9 (Size: 111):\n",
      "  - gender: 0.026\n",
      "  - transgender: 0.023\n",
      "  - trans: 0.019\n",
      "  - empathy: 0.018\n",
      "  - lgbtq: 0.018\n",
      "  - people: 0.016\n",
      "  - facts: 0.014\n",
      "  - let: 0.013\n",
      "  - decisions: 0.013\n",
      "  - choices: 0.013\n",
      "\n",
      "Topic 10 (Size: 82):\n",
      "  - energy: 0.078\n",
      "  - businesses: 0.042\n",
      "  - tax: 0.040\n",
      "  - costs: 0.031\n",
      "  - bills: 0.030\n",
      "  - taxes: 0.027\n",
      "  - energycrisis: 0.027\n",
      "  - energy costs: 0.027\n",
      "  - economy: 0.023\n",
      "  - dollar: 0.022\n",
      "\n",
      "Topic 11 (Size: 78):\n",
      "  - goodvibesonly: 0.062\n",
      "  - positivity: 0.051\n",
      "  - grateful: 0.050\n",
      "  - day: 0.047\n",
      "  - good: 0.043\n",
      "  - vibes: 0.040\n",
      "  - sending: 0.038\n",
      "  - good vibes: 0.036\n",
      "  - love: 0.033\n",
      "  - spreading: 0.029\n",
      "\n",
      "Topic 12 (Size: 75):\n",
      "  - cybersecurity: 0.088\n",
      "  - ai: 0.054\n",
      "  - security: 0.036\n",
      "  - ransomware: 0.034\n",
      "  - business: 0.031\n",
      "  - learn: 0.029\n",
      "  - secure: 0.028\n",
      "  - cyber: 0.026\n",
      "  - protect: 0.021\n",
      "  - intelligence: 0.019\n",
      "\n",
      "Topic 13 (Size: 73):\n",
      "  - assignments: 0.055\n",
      "  - essays: 0.055\n",
      "  - help: 0.052\n",
      "  - subjects: 0.040\n",
      "  - exams: 0.038\n",
      "  - need help: 0.035\n",
      "  - assistance: 0.035\n",
      "  - got covered: 0.032\n",
      "  - ve got: 0.031\n",
      "  - covered: 0.031\n",
      "\n",
      "Topic 14 (Size: 67):\n",
      "  - russia: 0.105\n",
      "  - ukraine: 0.088\n",
      "  - putin: 0.086\n",
      "  - russian: 0.045\n",
      "  - sanctions: 0.044\n",
      "  - war: 0.031\n",
      "  - uzbekistan: 0.025\n",
      "  - xi: 0.024\n",
      "  - geopolitics: 0.021\n",
      "  - military: 0.020\n",
      "\n",
      "Topic 15 (Size: 61):\n",
      "  - kardashian: 0.057\n",
      "  - fashion: 0.051\n",
      "  - makeup: 0.042\n",
      "  - kourtney: 0.040\n",
      "  - kourtney kardashian: 0.037\n",
      "  - hair: 0.036\n",
      "  - new: 0.029\n",
      "  - kim: 0.028\n",
      "  - business venture: 0.025\n",
      "  - venture: 0.023\n",
      "\n",
      "Topic 16 (Size: 55):\n",
      "  - unknown: 0.332\n",
      "  - unknown unknown: 0.158\n",
      "  - unknown alt: 0.151\n",
      "  - cop copglasgow: 0.139\n",
      "  - copglasgow: 0.138\n",
      "  - climateemergency scotradar: 0.137\n",
      "  - alt: 0.137\n",
      "  - seen times: 0.137\n",
      "  - scotradar cop: 0.137\n",
      "  - scotradar: 0.137\n",
      "\n",
      "Topic 17 (Size: 50):\n",
      "  - china: 0.088\n",
      "  - manufacturing: 0.043\n",
      "  - economy: 0.035\n",
      "  - tech: 0.034\n",
      "  - india: 0.033\n",
      "  - advanced tech: 0.025\n",
      "  - firms: 0.025\n",
      "  - trade: 0.024\n",
      "  - coronavirus: 0.023\n",
      "  - growth: 0.023\n",
      "\n",
      "Topic 18 (Size: 46):\n",
      "  - air: 0.149\n",
      "  - pollution: 0.128\n",
      "  - air pollution: 0.071\n",
      "  - pollution low: 0.048\n",
      "  - environment: 0.044\n",
      "  - air quality: 0.043\n",
      "  - clean: 0.043\n",
      "  - near: 0.041\n",
      "  - health: 0.035\n",
      "  - quality: 0.034\n"
     ]
    }
   ],
   "source": [
    "# 1. Save visualizations to HTML files\n",
    "fig = topic_model.visualize_barchart(top_n_topics=20)\n",
    "fig.write_html(\"../output/topic_barchart.html\")\n",
    "\n",
    "topic_model.visualize_topics().write_html(\"../output/topic_clusters.html\")\n",
    "topic_model.visualize_hierarchy().write_html(\"../output/topic_hierarchy.html\")\n",
    "\n",
    "# 2. Print text-based summary\n",
    "topics_info = topic_model.get_topic_info()\n",
    "print(\"\\nMost frequent topics with their terms:\")\n",
    "for _, row in topics_info.head(20).iterrows():\n",
    "    topic_id = row['Topic']\n",
    "    size = row['Count']\n",
    "    if topic_id != -1:\n",
    "        terms = topic_model.get_topic(topic_id)\n",
    "        print(f\"\\nTopic {topic_id} (Size: {size}):\")\n",
    "        # Print top 10 terms for each topic with their weights\n",
    "        for term, weight in terms[:20]:\n",
    "            print(f\"  - {term}: {weight:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add topic numbers and labels to dataframe\n",
    "df_posts['topic_num'] = topics\n",
    "\n",
    "# Get topic labels (using top 3 terms for each topic)\n",
    "topic_labels = {}\n",
    "for topic_id in set(topics):\n",
    "    if topic_id != -1:  # Skip outlier topic -1\n",
    "        terms = topic_model.get_topic(topic_id)\n",
    "        label = \", \".join([term for term, _ in terms[:3]])\n",
    "        topic_labels[topic_id] = label\n",
    "    else:\n",
    "        topic_labels[topic_id] = \"Other\"\n",
    "\n",
    "# Add descriptive topic labels\n",
    "df_posts['topic_label'] = df_posts['topic_num'].map(topic_labels)\n",
    "\n",
    "# Save augmented dataframe\n",
    "df_posts.to_csv('../output/posts_with_topics.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
