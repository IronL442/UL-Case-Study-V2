{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "1. Convert text to lowercase \n",
    "2. Remove URLs, mentions, and special characters (besides hashtags and emojis)\n",
    "3. Remove stop words\n",
    "4. Perform stemming/lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the spaCy model\n",
    "Can be installed via `python -m spacy download en_core_web_sm`\n",
    "\n",
    "Download Pre-Trained-Language Model: `wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import fasttext\n",
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file\n",
    "df_posts = pd.read_json('../data/dataset.json')\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info this can take over 30m!!\n",
    "# spacy.prefer_gpu()  # Prefers GPU but doesn't crash if unavailable\n",
    "# nlp = spacy.load(\"en_core_web_trf\")\n",
    "# def extract_entities(text):\n",
    "#     \"\"\"\n",
    "#     Extracts named entities from text using SpaCy's NER model.\n",
    "\n",
    "#     Args:\n",
    "#     text (str): The text from which to extract named entities.\n",
    "\n",
    "#     Returns:\n",
    "#     list: A list of tuples where each tuple contains (entity_text, entity_label).\n",
    "#     \"\"\"\n",
    "#     if not text or pd.isna(text):\n",
    "#         return []  # Return an empty list if text is missing\n",
    "\n",
    "#     # Process text with SpaCy\n",
    "#     doc = nlp(text)\n",
    "\n",
    "#     # Extract entity text and labels\n",
    "#     entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "#     return entities\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     \"\"\"\n",
    "#     Preprocesses text by removing URLs and emojis while keeping mentions and hashtags intact.\n",
    "\n",
    "#     Args:\n",
    "#     text (str): The original text.\n",
    "\n",
    "#     Returns:\n",
    "#     str: Preprocessed text.\n",
    "#     \"\"\"\n",
    "#     if not text or pd.isna(text):\n",
    "#         return \"\"  # Return empty string if text is missing\n",
    "\n",
    "#     # Remove URLs\n",
    "#     text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
    "#     # Remove emojis\n",
    "#     text = emoji.replace_emoji(text, replace=\"\")\n",
    "    \n",
    "#     return text.strip()\n",
    "\n",
    "# # Create a preprocessed text column\n",
    "# df_posts['preprocessed_text'] = df_posts['text'].apply(preprocess_text)\n",
    "\n",
    "# # Apply NER extraction on the preprocessed text\n",
    "# df_posts['entities'] = df_posts['preprocessed_text'].apply(extract_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for rows with no text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display rows where 'text' is missing (NaN)\n",
    "missing_text_rows = df_posts[df_posts['text'].isnull()]\n",
    "missing_text_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move hashtags to a new column 'hashtags'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply re.findall() to each row in the 'text' column to extract hashtags\n",
    "df_posts['hashtags'] = df_posts['text'].apply(lambda x: re.findall(r'#\\w+', x) if isinstance(x, str) else [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move mentions to a new column 'mentions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract mentions from the 'text' column, remove the '@' symbol, and create a new column 'mentions'\n",
    "df_posts['mentions'] = df_posts['text'].apply(lambda x: [mention[1:] for mention in re.findall(r'@\\w+', x)] if isinstance(x, str) else [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts['text'] = df_posts['text'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Date from Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Dates are the same\n",
    "unique_dates = df_posts['timestamp'].dt.date.unique()\n",
    "unique_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts['timestamp'] = df_posts['timestamp'].dt.time\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove URLs, Mentions, and Special Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave apostrophes in here for better lemmatization performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-compile regex patterns\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "    u\"\\U00002700-\\U000027BF\"  # dingbats\n",
    "    u\"\\U0001F900-\\U0001F9FF\"  # supplemental symbols and pictographs\n",
    "    u\"\\U00002600-\\U000026FF\"  # miscellaneous symbols\n",
    "    u\"\\U00002B50-\\U00002B55\"  # stars\n",
    "    \"]+\", flags=re.UNICODE)\n",
    "\n",
    "url_pattern = re.compile(r'http\\S+|www\\S+|https\\S+')  # Removes URLs\n",
    "mention_pattern = re.compile(r'@\\w+')  # Removes mentions\n",
    "punctuation_pattern = re.compile(r\"[^\\w\\s'â€™]\")  # Removes punctuation but keeps apostrophes\n",
    "number_pattern = re.compile(r'\\d+')  # Removes numbers\n",
    "whitespace_pattern = re.compile(r'\\s+')  # Removes excessive whitespace\n",
    "hashtag_pattern = re.compile(r'#\\w+')  # Removes hashtags and all text after them\n",
    "\n",
    "# Removes spaces between letters in a single word\n",
    "letter_spacing_pattern = re.compile(r'(\\b\\w)(?:\\s+)(\\w\\b)')\n",
    "\n",
    "# List of terms to remove\n",
    "remove_tw_terms = [\"cc\", \"cx\", \"ct\", \"dm\", \"ht\", \"mt\", \"prt\", \"rt\", \"followback\", \"follow back\", \"fb\", \"retweet\", \"retweets\"]\n",
    "\n",
    "# Compile regex to match terms (case insensitive and whole word)\n",
    "remove_terms_pattern = re.compile(r'\\b(' + '|'.join(remove_tw_terms) + r')\\b')\n",
    "\n",
    "# Updated regex for matching spaced-out letters (e.g., \"s h a r e\")\n",
    "letter_spacing_pattern = re.compile(r'(\\b(?:\\w\\s)+\\w\\b)')\n",
    "\n",
    "# Function to merge spaced-out letters\n",
    "def merge_spaced_letters(match):\n",
    "    # Remove spaces within the matched group\n",
    "    return match.group(0).replace(' ', '')\n",
    "\n",
    "def normalize_full_width(text):\n",
    "    # Convert full-width characters to half-width\n",
    "    return ''.join(\n",
    "        chr(ord(char) - 0xFEE0) if 0xFF01 <= ord(char) <= 0xFF5E else char\n",
    "        for char in text\n",
    "    )\n",
    "\n",
    "# Updated preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\", []  # Handle missing values gracefully\n",
    "    \n",
    "    # Normalize full-width characters\n",
    "    text = normalize_full_width(text)\n",
    "\n",
    "    # Extract emojis\n",
    "    emojis = emoji_pattern.findall(text)  # List of emojis\n",
    "\n",
    "    # Remove hashtags and text following them\n",
    "    text = hashtag_pattern.sub('', text)\n",
    "\n",
    "    # Remove emojis, URLs, mentions, punctuation, and numbers\n",
    "    text = emoji_pattern.sub('', text)  # Remove emojis\n",
    "    text = url_pattern.sub('', text)  # Remove URLs\n",
    "    text = mention_pattern.sub('', text)  # Remove mentions\n",
    "    text = punctuation_pattern.sub('', text)  # Remove punctuation\n",
    "    text = number_pattern.sub('', text)  # Remove numbers\n",
    "\n",
    "    # Remove specific terms (CC, CX, CT, DM, etc.)\n",
    "    text = remove_terms_pattern.sub('', text)\n",
    "\n",
    "    # Normalize letter spacing (e.g., \"s h a r e\" -> \"share\")\n",
    "    text = letter_spacing_pattern.sub(merge_spaced_letters, text)\n",
    "\n",
    "    # Remove excessive whitespace and trim\n",
    "    text = whitespace_pattern.sub(' ', text).strip()\n",
    "\n",
    "    return text, emojis\n",
    "\n",
    "# Apply preprocessing to create new columns\n",
    "df_posts[['text', 'emojis']] = df_posts['text'].apply(lambda x: pd.Series(preprocess_text(x)))\n",
    "\n",
    "# Display head to check the results\n",
    "df_posts[['text', 'emojis']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total duplicate rows\n",
    "total_duplicate_rows = df_posts['text'].duplicated(keep=False).sum()\n",
    "\n",
    "# Number of unique duplicate tweets\n",
    "unique_duplicate_tweets = (df_posts['text'].value_counts() > 1).sum()\n",
    "\n",
    "print(f\"Total duplicate rows: {total_duplicate_rows}\")\n",
    "print(f\"Unique duplicate tweets: {unique_duplicate_tweets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicate stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the frequency distribution of tweets\n",
    "frequency_distribution = df_posts['text'].value_counts()\n",
    "\n",
    "# Filter for only duplicates (frequency > 1)\n",
    "duplicate_tweet_frequencies = frequency_distribution[frequency_distribution > 1]\n",
    "\n",
    "# Summary statistics\n",
    "print(duplicate_tweet_frequencies.describe())\n",
    "\n",
    "# How many tweets are repeated more than 5 times?\n",
    "highly_duplicated = (duplicate_tweet_frequencies > 5).sum()\n",
    "print(f\"Number of tweets repeated more than 5 times: {highly_duplicated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicate tweets only if the same person posted the same tweet (spam) but extract frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Group by 'user' and 'text' and calculate the frequency of each combination\n",
    "df_posts['frequency'] = df_posts.groupby(['user', 'text'])['text'].transform('count')\n",
    "\n",
    "# Step 2: Drop duplicates based on 'user' and 'text' (keeping the first occurrence)\n",
    "df_posts = df_posts.drop_duplicates(subset=['user', 'text'], keep='first').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts[df_posts['user'] == 'reginabarnes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indentify language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained language identification model\n",
    "model = fasttext.load_model('../models/lid.176.bin')\n",
    "\n",
    "# Function to detect language using FastText\n",
    "def detect_language_fasttext(text, threshold=0.8):\n",
    "    # Skip tweets with fewer than 3 words\n",
    "    if len(text.split()) < 3:\n",
    "        return None  # Skip these tweets entirely\n",
    "    try:\n",
    "        predictions = model.predict(text)  # Predict the language\n",
    "        lang_code = predictions[0][0].replace(\"__label__\", \"\")  # Extract language code\n",
    "        return lang_code\n",
    "    except Exception as e:\n",
    "        return None  # Skip on exception\n",
    "\n",
    "# Apply language detection to the 'text' column\n",
    "df_posts['language'] = df_posts['text'].apply(detect_language_fasttext)\n",
    "\n",
    "# Filter non-English tweets (ignoring None values)\n",
    "non_english_tweets = df_posts[(df_posts['language'].notna()) & (df_posts['language'] != 'en')]\n",
    "\n",
    "# Filter to keep English tweets AND tweets with None language\n",
    "df_posts = df_posts[(df_posts['language'] == 'en') | (df_posts['language'].isna())].copy()\n",
    "\n",
    "# Print diagnostic information\n",
    "print(f\"Total tweets after filtering: {len(df_posts)}\")\n",
    "print(f\"English tweets: {len(df_posts[df_posts['language'] == 'en'])}\")\n",
    "print(f\"Tweets with None language: {df_posts['language'].isna().sum()}\")\n",
    "print(f\"Number of non-English tweets: {len(non_english_tweets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the non-English tweets to a CSV file\n",
    "non_english_tweets.to_csv('../output/non_english_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of appearances of each Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_counts = non_english_tweets['language'].value_counts()\n",
    "print(\"Number of tweets per non-english language:\")\n",
    "language_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translate the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize translator\n",
    "translator = Translator()\n",
    "\n",
    "languages_to_translate = ['pt', 'es', 'th', 'id']  # List of language codes\n",
    "translated_tweets = non_english_tweets[non_english_tweets['language'].isin(languages_to_translate)]\n",
    "\n",
    "# Function to translate text\n",
    "def translate_text(text):\n",
    "    try:\n",
    "        return translator.translate(text, dest='en').text\n",
    "    except Exception as e:\n",
    "        return text  # Return original text if translation fails\n",
    "\n",
    "# Translate the tweets in the filtered DataFrame\n",
    "translated_tweets['text'] = translated_tweets['text'].apply(translate_text)\n",
    "\n",
    "# Save the translated tweets to a CSV file\n",
    "translated_tweets.to_csv('../output/translated_tweets.csv', index=False)\n",
    "\n",
    "# Display the number of translated tweets\n",
    "print(f\"Number of tweets translated: {len(translated_tweets)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append translated tweets to english tweets dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts = pd.concat([df_posts, translated_tweets], ignore_index=True)\n",
    "\n",
    "print(f\"Final number of tweets: {len(df_posts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export for Topic Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.to_csv('../output/export_for_topic_classification.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load en_core_web_sm for spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use lemmatization since stemming can lead to less accurate results (even non-words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info this takes about 4.5m!!\n",
    "df_posts['text'] = df_posts['text'].apply(\n",
    "        lambda text: ' '.join([token.lemma_ for token in nlp(text)])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create output for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.to_csv('../output/preprocessed_for_SA.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts['text'] = df_posts['text'].fillna(\"\").apply(\n",
    "    lambda text: ' '.join([token.text for token in nlp.make_doc(text) if not token.is_stop])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Apostrophes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts['text'] = df_posts['text'].str.replace(r\"[â€™']\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check again for empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display rows where 'text' is missing (NaN)\n",
    "missing_text_rows = df_posts[df_posts['text'].isnull()]\n",
    "missing_text_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the output to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.to_csv('../output/preprocessed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
