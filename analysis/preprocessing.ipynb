{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "1. Convert text to lowercase \n",
    "2. Remove URLs, mentions, and special characters (besides hashtags and emojis)\n",
    "3. Remove stop words\n",
    "4. Perform stemming/lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the spaCy model\n",
    "Can be installed via `python -m spacy download en_core_web_sm`\n",
    "\n",
    "Download Pre-Trained-Language Model: `wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import fasttext\n",
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>text_id</th>\n",
       "      <th>user</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>Running a business means juggling countless ad...</td>\n",
       "      <td>2018569761</td>\n",
       "      <td>danielwoodard</td>\n",
       "      <td>1077866112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>Liz Truss is walking in the lingering shadow o...</td>\n",
       "      <td>2092717718</td>\n",
       "      <td>nelsonjacqueline</td>\n",
       "      <td>1089670430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>The UK is bracing for war as government buildi...</td>\n",
       "      <td>2059143248</td>\n",
       "      <td>ihooper</td>\n",
       "      <td>1007478642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>Marrying a second or third cousin once removed...</td>\n",
       "      <td>2008209828</td>\n",
       "      <td>wrightnicholas</td>\n",
       "      <td>1039258480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>It's truly disgraceful how the Indian National...</td>\n",
       "      <td>2001239278</td>\n",
       "      <td>michael51</td>\n",
       "      <td>1021455936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp                                               text     text_id  \\\n",
       "0 2024-10-31  Running a business means juggling countless ad...  2018569761   \n",
       "1 2024-10-31  Liz Truss is walking in the lingering shadow o...  2092717718   \n",
       "2 2024-10-31  The UK is bracing for war as government buildi...  2059143248   \n",
       "3 2024-10-31  Marrying a second or third cousin once removed...  2008209828   \n",
       "4 2024-10-31  It's truly disgraceful how the Indian National...  2001239278   \n",
       "\n",
       "               user     user_id  \n",
       "0     danielwoodard  1077866112  \n",
       "1  nelsonjacqueline  1089670430  \n",
       "2           ihooper  1007478642  \n",
       "3    wrightnicholas  1039258480  \n",
       "4         michael51  1021455936  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the JSON file\n",
    "df_posts = pd.read_json('../data/dataset.json')\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info this can take over 30m!!\n",
    "# spacy.prefer_gpu()  # Prefers GPU but doesn't crash if unavailable\n",
    "# nlp = spacy.load(\"en_core_web_trf\")\n",
    "# def extract_entities(text):\n",
    "#     \"\"\"\n",
    "#     Extracts named entities from text using SpaCy's NER model.\n",
    "\n",
    "#     Args:\n",
    "#     text (str): The text from which to extract named entities.\n",
    "\n",
    "#     Returns:\n",
    "#     list: A list of tuples where each tuple contains (entity_text, entity_label).\n",
    "#     \"\"\"\n",
    "#     if not text or pd.isna(text):\n",
    "#         return []  # Return an empty list if text is missing\n",
    "\n",
    "#     # Process text with SpaCy\n",
    "#     doc = nlp(text)\n",
    "\n",
    "#     # Extract entity text and labels\n",
    "#     entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "#     return entities\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     \"\"\"\n",
    "#     Preprocesses text by removing URLs and emojis while keeping mentions and hashtags intact.\n",
    "\n",
    "#     Args:\n",
    "#     text (str): The original text.\n",
    "\n",
    "#     Returns:\n",
    "#     str: Preprocessed text.\n",
    "#     \"\"\"\n",
    "#     if not text or pd.isna(text):\n",
    "#         return \"\"  # Return empty string if text is missing\n",
    "\n",
    "#     # Remove URLs\n",
    "#     text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
    "#     # Remove emojis\n",
    "#     text = emoji.replace_emoji(text, replace=\"\")\n",
    "    \n",
    "#     return text.strip()\n",
    "\n",
    "# # Create a preprocessed text column\n",
    "# df_posts['preprocessed_text'] = df_posts['text'].apply(preprocess_text)\n",
    "\n",
    "# # Apply NER extraction on the preprocessed text\n",
    "# df_posts['entities'] = df_posts['preprocessed_text'].apply(extract_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for rows with no text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>text_id</th>\n",
       "      <th>user</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [timestamp, text, text_id, user, user_id]\n",
       "Index: []"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display rows where 'text' is missing (NaN)\n",
    "missing_text_rows = df_posts[df_posts['text'].isnull()]\n",
    "missing_text_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move hashtags to a new column 'hashtags'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply re.findall() to each row in the 'text' column to extract hashtags\n",
    "df_posts['hashtags'] = df_posts['text'].apply(lambda x: re.findall(r'#\\w+', x) if isinstance(x, str) else [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move mentions to a new column 'mentions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract mentions from the 'text' column, remove the '@' symbol, and create a new column 'mentions'\n",
    "df_posts['mentions'] = df_posts['text'].apply(lambda x: [mention[1:] for mention in re.findall(r'@\\w+', x)] if isinstance(x, str) else [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts['text'] = df_posts['text'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Date from Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([datetime.date(2024, 10, 31)], dtype=object)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All Dates are the same\n",
    "unique_dates = df_posts['timestamp'].dt.date.unique()\n",
    "unique_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>text_id</th>\n",
       "      <th>user</th>\n",
       "      <th>user_id</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00:00:00</td>\n",
       "      <td>running a business means juggling countless ad...</td>\n",
       "      <td>2018569761</td>\n",
       "      <td>danielwoodard</td>\n",
       "      <td>1077866112</td>\n",
       "      <td>[#HRtech, #businessmanagement]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00:00:00</td>\n",
       "      <td>liz truss is walking in the lingering shadow o...</td>\n",
       "      <td>2092717718</td>\n",
       "      <td>nelsonjacqueline</td>\n",
       "      <td>1089670430</td>\n",
       "      <td>[#politics]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00:00:00</td>\n",
       "      <td>the uk is bracing for war as government buildi...</td>\n",
       "      <td>2059143248</td>\n",
       "      <td>ihooper</td>\n",
       "      <td>1007478642</td>\n",
       "      <td>[#Ukrainewashed, #WarPreparedness]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00:00:00</td>\n",
       "      <td>marrying a second or third cousin once removed...</td>\n",
       "      <td>2008209828</td>\n",
       "      <td>wrightnicholas</td>\n",
       "      <td>1039258480</td>\n",
       "      <td>[#FamilyTree, #GeneticFacts]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00:00:00</td>\n",
       "      <td>it's truly disgraceful how the indian national...</td>\n",
       "      <td>2001239278</td>\n",
       "      <td>michael51</td>\n",
       "      <td>1021455936</td>\n",
       "      <td>[#RationChorCongress]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  timestamp                                               text     text_id  \\\n",
       "0  00:00:00  running a business means juggling countless ad...  2018569761   \n",
       "1  00:00:00  liz truss is walking in the lingering shadow o...  2092717718   \n",
       "2  00:00:00  the uk is bracing for war as government buildi...  2059143248   \n",
       "3  00:00:00  marrying a second or third cousin once removed...  2008209828   \n",
       "4  00:00:00  it's truly disgraceful how the indian national...  2001239278   \n",
       "\n",
       "               user     user_id                            hashtags mentions  \n",
       "0     danielwoodard  1077866112      [#HRtech, #businessmanagement]       []  \n",
       "1  nelsonjacqueline  1089670430                         [#politics]       []  \n",
       "2           ihooper  1007478642  [#Ukrainewashed, #WarPreparedness]       []  \n",
       "3    wrightnicholas  1039258480        [#FamilyTree, #GeneticFacts]       []  \n",
       "4         michael51  1021455936               [#RationChorCongress]       []  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_posts['timestamp'] = df_posts['timestamp'].dt.time\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove URLs, Mentions, and Special Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave apostrophes in here for better lemmatization performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emojis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>running a business means juggling countless ad...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>liz truss is walking in the lingering shadow o...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the uk is bracing for war as government buildi...</td>\n",
       "      <td>[ðŸ‡ºðŸ‡¦]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>marrying a second or third cousin once removed...</td>\n",
       "      <td>[ðŸ§¬]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>it's truly disgraceful how the indian national...</td>\n",
       "      <td>[ðŸ¤¦, â™‚]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  emojis\n",
       "0  running a business means juggling countless ad...      []\n",
       "1  liz truss is walking in the lingering shadow o...      []\n",
       "2  the uk is bracing for war as government buildi...    [ðŸ‡ºðŸ‡¦]\n",
       "3  marrying a second or third cousin once removed...     [ðŸ§¬]\n",
       "4  it's truly disgraceful how the indian national...  [ðŸ¤¦, â™‚]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre-compile regex patterns\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "    u\"\\U00002700-\\U000027BF\"  # dingbats\n",
    "    u\"\\U0001F900-\\U0001F9FF\"  # supplemental symbols and pictographs\n",
    "    u\"\\U00002600-\\U000026FF\"  # miscellaneous symbols\n",
    "    u\"\\U00002B50-\\U00002B55\"  # stars\n",
    "    \"]+\", flags=re.UNICODE)\n",
    "\n",
    "url_pattern = re.compile(r'http\\S+|www\\S+|https\\S+')  # Removes URLs\n",
    "mention_pattern = re.compile(r'@\\w+')  # Removes mentions\n",
    "punctuation_pattern = re.compile(r\"[^\\w\\s'â€™]\")  # Removes punctuation but keeps apostrophes\n",
    "number_pattern = re.compile(r'\\d+')  # Removes numbers\n",
    "whitespace_pattern = re.compile(r'\\s+')  # Removes excessive whitespace\n",
    "hashtag_pattern = re.compile(r'#\\w+')  # Removes hashtags and all text after them\n",
    "\n",
    "# Removes spaces between letters in a single word\n",
    "letter_spacing_pattern = re.compile(r'(\\b\\w)(?:\\s+)(\\w\\b)')\n",
    "\n",
    "# List of terms to remove\n",
    "remove_tw_terms = [\"cc\", \"cx\", \"ct\", \"dm\", \"ht\", \"mt\", \"prt\", \"rt\", \"followback\", \"follow back\", \"fb\", \"retweet\", \"retweets\"]\n",
    "\n",
    "# Compile regex to match terms (case insensitive and whole word)\n",
    "remove_terms_pattern = re.compile(r'\\b(' + '|'.join(remove_tw_terms) + r')\\b')\n",
    "\n",
    "# Updated regex for matching spaced-out letters (e.g., \"s h a r e\")\n",
    "letter_spacing_pattern = re.compile(r'(\\b(?:\\w\\s)+\\w\\b)')\n",
    "\n",
    "# Function to merge spaced-out letters\n",
    "def merge_spaced_letters(match):\n",
    "    # Remove spaces within the matched group\n",
    "    return match.group(0).replace(' ', '')\n",
    "\n",
    "def normalize_full_width(text):\n",
    "    # Convert full-width characters to half-width\n",
    "    return ''.join(\n",
    "        chr(ord(char) - 0xFEE0) if 0xFF01 <= ord(char) <= 0xFF5E else char\n",
    "        for char in text\n",
    "    )\n",
    "\n",
    "# Updated preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\", []  # Handle missing values gracefully\n",
    "    \n",
    "    # Normalize full-width characters\n",
    "    text = normalize_full_width(text)\n",
    "\n",
    "    # Extract emojis\n",
    "    emojis = emoji_pattern.findall(text)  # List of emojis\n",
    "\n",
    "    # Remove hashtags and text following them\n",
    "    text = hashtag_pattern.sub('', text)\n",
    "\n",
    "    # Remove emojis, URLs, mentions, punctuation, and numbers\n",
    "    text = emoji_pattern.sub('', text)  # Remove emojis\n",
    "    text = url_pattern.sub('', text)  # Remove URLs\n",
    "    text = mention_pattern.sub('', text)  # Remove mentions\n",
    "    text = punctuation_pattern.sub('', text)  # Remove punctuation\n",
    "    text = number_pattern.sub('', text)  # Remove numbers\n",
    "\n",
    "    # Remove specific terms (CC, CX, CT, DM, etc.)\n",
    "    text = remove_terms_pattern.sub('', text)\n",
    "\n",
    "    # Normalize letter spacing (e.g., \"s h a r e\" -> \"share\")\n",
    "    text = letter_spacing_pattern.sub(merge_spaced_letters, text)\n",
    "\n",
    "    # Remove excessive whitespace and trim\n",
    "    text = whitespace_pattern.sub(' ', text).strip()\n",
    "\n",
    "    return text, emojis\n",
    "\n",
    "# Apply preprocessing to create new columns\n",
    "df_posts[['text', 'emojis']] = df_posts['text'].apply(lambda x: pd.Series(preprocess_text(x)))\n",
    "\n",
    "# Display head to check the results\n",
    "df_posts[['text', 'emojis']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.to_csv('../output/testing.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duplicate rows: 40239\n",
      "Unique duplicate tweets: 16405\n"
     ]
    }
   ],
   "source": [
    "# Total duplicate rows\n",
    "total_duplicate_rows = df_posts['text'].duplicated(keep=False).sum()\n",
    "\n",
    "# Number of unique duplicate tweets\n",
    "unique_duplicate_tweets = (df_posts['text'].value_counts() > 1).sum()\n",
    "\n",
    "print(f\"Total duplicate rows: {total_duplicate_rows}\")\n",
    "print(f\"Unique duplicate tweets: {unique_duplicate_tweets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicate stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    16405.000000\n",
      "mean         2.452850\n",
      "std          2.188017\n",
      "min          2.000000\n",
      "25%          2.000000\n",
      "50%          2.000000\n",
      "75%          3.000000\n",
      "max        130.000000\n",
      "Name: count, dtype: float64\n",
      "Number of tweets repeated more than 5 times: 150\n"
     ]
    }
   ],
   "source": [
    "# Get the frequency distribution of tweets\n",
    "frequency_distribution = df_posts['text'].value_counts()\n",
    "\n",
    "# Filter for only duplicates (frequency > 1)\n",
    "duplicate_tweet_frequencies = frequency_distribution[frequency_distribution > 1]\n",
    "\n",
    "# Summary statistics\n",
    "print(duplicate_tweet_frequencies.describe())\n",
    "\n",
    "# How many tweets are repeated more than 5 times?\n",
    "highly_duplicated = (duplicate_tweet_frequencies > 5).sum()\n",
    "print(f\"Number of tweets repeated more than 5 times: {highly_duplicated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicate tweets only if the same person posted the same tweet (spam) but extract frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Group by 'user' and 'text' and calculate the frequency of each combination\n",
    "df_posts['frequency'] = df_posts.groupby(['user', 'text'])['text'].transform('count')\n",
    "\n",
    "# Step 2: Drop duplicates based on 'user' and 'text' (keeping the first occurrence)\n",
    "df_posts = df_posts.drop_duplicates(subset=['user', 'text'], keep='first').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>text_id</th>\n",
       "      <th>user</th>\n",
       "      <th>user_id</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>emojis</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>00:00:19</td>\n",
       "      <td>wwf no mercy usa variant nintendo n cart only ...</td>\n",
       "      <td>2053226516</td>\n",
       "      <td>reginabarnes</td>\n",
       "      <td>1068051022</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp                                               text     text_id  \\\n",
       "48  00:00:19  wwf no mercy usa variant nintendo n cart only ...  2053226516   \n",
       "\n",
       "            user     user_id hashtags mentions emojis  frequency  \n",
       "48  reginabarnes  1068051022       []       []     []         86  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_posts[df_posts['user'] == 'reginabarnes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indentify language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-English tweets: 865\n",
      "                                                    text language\n",
      "48     wwf no mercy usa variant nintendo n cart only ...       hu\n",
      "187    votem sempre na tag se deixem decidir o futuro...       pt\n",
      "256    aunque ya no puedo votar en otras plataformas ...       es\n",
      "330    vote no site e na tag escutem no app e deem vi...       pt\n",
      "529    when life gives you lemons throw them back and...       th\n",
      "...                                                  ...      ...\n",
      "68273  mil eo jungkook vai cantar no seu aniversÃ¡rio ...       pt\n",
      "68365                              la bellezza Ã¨ ovunque       it\n",
      "68387  se esse tweet chegar a o jungkook vai fazer um...       pt\n",
      "68451  a votaÃ§Ã£o tÃ¡ on votem na hashtag e escutem as ...       pt\n",
      "68514  o que Ã© isso a katy perry se aposentou acredit...       pt\n",
      "\n",
      "[865 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained language identification model\n",
    "model = fasttext.load_model('../models/lid.176.bin')\n",
    "\n",
    "# Function to detect language using FastText\n",
    "def detect_language_fasttext(text, threshold=0.8):\n",
    "    # Skip tweets with fewer than 3 words\n",
    "    if len(text.split()) < 3:\n",
    "        return None  # Skip these tweets entirely\n",
    "    try:\n",
    "        predictions = model.predict(text)  # Predict the language\n",
    "        lang_code = predictions[0][0].replace(\"__label__\", \"\")  # Extract language code\n",
    "        return lang_code\n",
    "    except Exception as e:\n",
    "        return None  # Skip on exception\n",
    "\n",
    "# Apply language detection to the 'text' column\n",
    "df_posts['language'] = df_posts['text'].apply(detect_language_fasttext)\n",
    "\n",
    "# Filter non-English tweets (ignoring None values)\n",
    "non_english_tweets = df_posts[(df_posts['language'].notna()) & (df_posts['language'] != 'en')]\n",
    "\n",
    "# Display the number of non-English tweets and a sample\n",
    "print(f\"Number of non-English tweets: {len(non_english_tweets)}\")\n",
    "print(non_english_tweets[['text', 'language']])\n",
    "\n",
    "# Save the non-English tweets to a CSV file\n",
    "non_english_tweets.to_csv('../output/non_english_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of appearances of each Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets per non-english language:\n",
      "language\n",
      "pt     546\n",
      "es     143\n",
      "th      74\n",
      "id      17\n",
      "sw      11\n",
      "de      10\n",
      "tl       9\n",
      "it       5\n",
      "ko       4\n",
      "ta       4\n",
      "fr       3\n",
      "ar       3\n",
      "fi       3\n",
      "pl       3\n",
      "ms       3\n",
      "tr       3\n",
      "ja       2\n",
      "ru       2\n",
      "nap      2\n",
      "bn       2\n",
      "lt       2\n",
      "ca       2\n",
      "no       2\n",
      "hi       2\n",
      "nl       2\n",
      "mr       1\n",
      "uz       1\n",
      "hu       1\n",
      "sv       1\n",
      "ml       1\n",
      "la       1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "language_counts = non_english_tweets['language'].value_counts()\n",
    "\n",
    "print(\"Number of tweets per non-english language:\")\n",
    "print(language_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translate the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets translated: 780\n",
      "Translated tweets saved to: ../output/translated_tweets.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m6/2lgmm8516r37yq5gnyyd84g00000gn/T/ipykernel_25383/754318150.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_tweets['translated_text'] = filtered_tweets['text'].apply(translate_text)\n"
     ]
    }
   ],
   "source": [
    "# Initialize translator\n",
    "translator = Translator()\n",
    "\n",
    "languages_to_translate = ['pt', 'es', 'th', 'id']  # List of language codes\n",
    "filtered_tweets = non_english_tweets[non_english_tweets['language'].isin(languages_to_translate)]\n",
    "\n",
    "# Function to translate text\n",
    "def translate_text(text):\n",
    "    try:\n",
    "        return translator.translate(text, dest='en').text\n",
    "    except Exception as e:\n",
    "        return text  # Return original text if translation fails\n",
    "\n",
    "# Translate the tweets in the filtered DataFrame\n",
    "filtered_tweets['translated_text'] = filtered_tweets['text'].apply(translate_text)\n",
    "\n",
    "# Save the translated tweets to a CSV file\n",
    "output_path = '../output/translated_tweets.csv'\n",
    "filtered_tweets.to_csv(output_path, index=False)\n",
    "\n",
    "# Display the number of translated tweets\n",
    "print(f\"Number of tweets translated: {len(filtered_tweets)}\")\n",
    "print(f\"Translated tweets saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load en_core_web_sm for spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use lemmatization since stemming can lead to less accurate results (even non-words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[131], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Info this takes about 2.5m!!\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_posts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_posts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemma_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/case-study-ul/venv/lib/python3.12/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/case-study-ul/venv/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/case-study-ul/venv/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/Documents/GitHub/case-study-ul/venv/lib/python3.12/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/case-study-ul/venv/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[131], line 3\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Info this takes about 2.5m!!\u001b[39;00m\n\u001b[1;32m      2\u001b[0m df_posts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_posts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m----> 3\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m text: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([token\u001b[38;5;241m.\u001b[39mlemma_ \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m])\n\u001b[1;32m      4\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/GitHub/case-study-ul/venv/lib/python3.12/site-packages/spacy/language.py:1052\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1052\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1054\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/case-study-ul/venv/lib/python3.12/site-packages/spacy/pipeline/trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/GitHub/case-study-ul/venv/lib/python3.12/site-packages/spacy/pipeline/transition_parser.pyx:264\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.predict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/GitHub/case-study-ul/venv/lib/python3.12/site-packages/spacy/pipeline/transition_parser.pyx:285\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.greedy_parse\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/GitHub/case-study-ul/venv/lib/python3.12/site-packages/thinc/model.py:334\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutT:\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/GitHub/case-study-ul/venv/lib/python3.12/site-packages/spacy/ml/tb_framework.py:34\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(model, X, is_train):\n\u001b[0;32m---> 34\u001b[0m     step_model \u001b[38;5;241m=\u001b[39m \u001b[43mParserStepModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43munseen_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munseen_classes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_upper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhas_upper\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m step_model, step_model\u001b[38;5;241m.\u001b[39mfinish_steps\n",
      "File \u001b[0;32m~/Documents/GitHub/case-study-ul/venv/lib/python3.12/site-packages/spacy/ml/parser_model.pyx:250\u001b[0m, in \u001b[0;36mspacy.ml.parser_model.ParserStepModel.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/GitHub/case-study-ul/venv/lib/python3.12/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/case-study-ul/venv/lib/python3.12/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/Documents/GitHub/case-study-ul/venv/lib/python3.12/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/case-study-ul/venv/lib/python3.12/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/Documents/GitHub/case-study-ul/venv/lib/python3.12/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/case-study-ul/venv/lib/python3.12/site-packages/thinc/layers/with_array.py:42\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, Xseq, is_train)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m](Xseq, is_train)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], \u001b[43m_list_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/GitHub/case-study-ul/venv/lib/python3.12/site-packages/thinc/layers/with_array.py:77\u001b[0m, in \u001b[0;36m_list_forward\u001b[0;34m(model, Xs, is_train)\u001b[0m\n\u001b[1;32m     75\u001b[0m lengths \u001b[38;5;241m=\u001b[39m NUMPY_OPS\u001b[38;5;241m.\u001b[39masarray1i([\u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m Xs])\n\u001b[1;32m     76\u001b[0m Xf \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mflatten(Xs, pad\u001b[38;5;241m=\u001b[39mpad)\n\u001b[0;32m---> 77\u001b[0m Yf, get_dXf \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbackprop\u001b[39m(dYs: ListXd) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ListXd:\n\u001b[1;32m     80\u001b[0m     dYf \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mflatten(dYs, pad\u001b[38;5;241m=\u001b[39mpad)\n",
      "File \u001b[0;32m~/Documents/GitHub/case-study-ul/venv/lib/python3.12/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/case-study-ul/venv/lib/python3.12/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/Documents/GitHub/case-study-ul/venv/lib/python3.12/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/case-study-ul/venv/lib/python3.12/site-packages/thinc/layers/residual.py:41\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m d_output \u001b[38;5;241m+\u001b[39m dX\n\u001b[0;32m---> 41\u001b[0m Y, backprop_layer \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [X[i] \u001b[38;5;241m+\u001b[39m Y[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X))], backprop\n",
      "File \u001b[0;32m~/Documents/GitHub/case-study-ul/venv/lib/python3.12/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/case-study-ul/venv/lib/python3.12/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/Documents/GitHub/case-study-ul/venv/lib/python3.12/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/case-study-ul/venv/lib/python3.12/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "    \u001b[0;31m[... skipping similar frames: Model.__call__ at line 310 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/GitHub/case-study-ul/venv/lib/python3.12/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/Documents/GitHub/case-study-ul/venv/lib/python3.12/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/case-study-ul/venv/lib/python3.12/site-packages/thinc/layers/maxout.py:52\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     50\u001b[0m W \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_param(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m W \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape2f(W, nO \u001b[38;5;241m*\u001b[39m nP, nI)\n\u001b[0;32m---> 52\u001b[0m Y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgemm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m Y \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape1f(b, nO \u001b[38;5;241m*\u001b[39m nP)\n\u001b[1;32m     54\u001b[0m Z \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape3f(Y, Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], nO, nP)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Info this takes about 2.5m!!\n",
    "df_posts['text'] = df_posts['text'].apply(\n",
    "        lambda text: ' '.join([token.lemma_ for token in nlp(text)])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create output for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.to_csv('../output/preprocessed_for_SA.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts['text'] = df_posts['text'].fillna(\"\").apply(\n",
    "    lambda text: ' '.join([token.text for token in nlp.make_doc(text) if not token.is_stop])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Apostrophes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts['text'] = df_posts['text'].str.replace(r\"[â€™']\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check again for empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>text_id</th>\n",
       "      <th>user</th>\n",
       "      <th>user_id</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>emojis</th>\n",
       "      <th>frequency</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [timestamp, text, text_id, user, user_id, hashtags, mentions, emojis, frequency, language]\n",
       "Index: []"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display rows where 'text' is missing (NaN)\n",
    "missing_text_rows = df_posts[df_posts['text'].isnull()]\n",
    "missing_text_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the output to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.to_csv('../output/preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>text_id</th>\n",
       "      <th>user</th>\n",
       "      <th>user_id</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>emojis</th>\n",
       "      <th>frequency</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>00:00:19</td>\n",
       "      <td>wwf mercy usa variant nintendo n cart mint</td>\n",
       "      <td>2053226516</td>\n",
       "      <td>reginabarnes</td>\n",
       "      <td>1068051022</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>86</td>\n",
       "      <td>hu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10602</th>\n",
       "      <td>02:57:00</td>\n",
       "      <td>climate change effective way stop tsunami torn...</td>\n",
       "      <td>2016959704</td>\n",
       "      <td>smithcynthia</td>\n",
       "      <td>1055454345</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>116</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45453</th>\n",
       "      <td>12:32:46</td>\n",
       "      <td>difference degree global warming</td>\n",
       "      <td>2011488077</td>\n",
       "      <td>raymondberry</td>\n",
       "      <td>1031499948</td>\n",
       "      <td>[#ClimateCrisis, #ClimateEmergency, #ClimateAc...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>119</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      timestamp                                               text  \\\n",
       "48     00:00:19         wwf mercy usa variant nintendo n cart mint   \n",
       "10602  02:57:00  climate change effective way stop tsunami torn...   \n",
       "45453  12:32:46                   difference degree global warming   \n",
       "\n",
       "          text_id          user     user_id  \\\n",
       "48     2053226516  reginabarnes  1068051022   \n",
       "10602  2016959704  smithcynthia  1055454345   \n",
       "45453  2011488077  raymondberry  1031499948   \n",
       "\n",
       "                                                hashtags mentions emojis  \\\n",
       "48                                                    []       []     []   \n",
       "10602                                                 []       []     []   \n",
       "45453  [#ClimateCrisis, #ClimateEmergency, #ClimateAc...       []     []   \n",
       "\n",
       "       frequency language  \n",
       "48            86       hu  \n",
       "10602        116       en  \n",
       "45453        119       en  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_posts[df_posts['frequency'] > 85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
