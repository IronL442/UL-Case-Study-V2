{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "1. Convert text to lowercase \n",
    "2. Remove URLs, mentions, and special characters (besides hashtags and emojis)\n",
    "3. Remove stop words\n",
    "4. Perform stemming/lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from googletrans import Translator\n",
    "import fasttext\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>text_id</th>\n",
       "      <th>user</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>Running a business means juggling countless ad...</td>\n",
       "      <td>2018569761</td>\n",
       "      <td>danielwoodard</td>\n",
       "      <td>1077866112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>Liz Truss is walking in the lingering shadow o...</td>\n",
       "      <td>2092717718</td>\n",
       "      <td>nelsonjacqueline</td>\n",
       "      <td>1089670430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>The UK is bracing for war as government buildi...</td>\n",
       "      <td>2059143248</td>\n",
       "      <td>ihooper</td>\n",
       "      <td>1007478642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>Marrying a second or third cousin once removed...</td>\n",
       "      <td>2008209828</td>\n",
       "      <td>wrightnicholas</td>\n",
       "      <td>1039258480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>It's truly disgraceful how the Indian National...</td>\n",
       "      <td>2001239278</td>\n",
       "      <td>michael51</td>\n",
       "      <td>1021455936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp                                               text     text_id  \\\n",
       "0 2024-10-31  Running a business means juggling countless ad...  2018569761   \n",
       "1 2024-10-31  Liz Truss is walking in the lingering shadow o...  2092717718   \n",
       "2 2024-10-31  The UK is bracing for war as government buildi...  2059143248   \n",
       "3 2024-10-31  Marrying a second or third cousin once removed...  2008209828   \n",
       "4 2024-10-31  It's truly disgraceful how the Indian National...  2001239278   \n",
       "\n",
       "               user     user_id  \n",
       "0     danielwoodard  1077866112  \n",
       "1  nelsonjacqueline  1089670430  \n",
       "2           ihooper  1007478642  \n",
       "3    wrightnicholas  1039258480  \n",
       "4         michael51  1021455936  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the JSON file\n",
    "df_posts = pd.read_json('../data/dataset.json')\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for rows with no text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>text_id</th>\n",
       "      <th>user</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [timestamp, text, text_id, user, user_id]\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display rows where 'text' is missing (NaN)\n",
    "missing_text_rows = df_posts[df_posts['text'].isnull()]\n",
    "missing_text_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Unicode emoji with emoji name\n",
    "def replace_emojis(text):\n",
    "    return emoji.demojize(text, delimiters='')\n",
    "\n",
    "# Remove all emojis\n",
    "def remove_emojis(text):\n",
    "    return emoji.replace_emoji(text, replace='')\n",
    "\n",
    "\n",
    "#df_posts['text'] = df_posts['text'].apply(replace_emojis)\n",
    "df_posts['text'] = df_posts['text'].apply(remove_emojis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy hashtags to a new column 'hashtags'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply regex to each row in the 'text' column to extract hashtags\n",
    "df_posts['hashtags'] = df_posts['text'].apply(lambda x: re.findall(r'#\\w+', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy mentions to a new column 'mentions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract mentions from the 'text' column, remove the '@' symbol, and create a new column 'mentions'\n",
    "df_posts['mentions'] = df_posts['text'].apply(lambda x: [mention[1:] for mention in re.findall(r'@\\w+', x)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Date from Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([datetime.date(2024, 10, 31)], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All Dates are the same\n",
    "unique_dates = df_posts['timestamp'].dt.date.unique()\n",
    "unique_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>text_id</th>\n",
       "      <th>user</th>\n",
       "      <th>user_id</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00:00:00</td>\n",
       "      <td>Running a business means juggling countless ad...</td>\n",
       "      <td>2018569761</td>\n",
       "      <td>danielwoodard</td>\n",
       "      <td>1077866112</td>\n",
       "      <td>[#HRtech, #businessmanagement]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00:00:00</td>\n",
       "      <td>Liz Truss is walking in the lingering shadow o...</td>\n",
       "      <td>2092717718</td>\n",
       "      <td>nelsonjacqueline</td>\n",
       "      <td>1089670430</td>\n",
       "      <td>[#politics]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00:00:00</td>\n",
       "      <td>The UK is bracing for war as government buildi...</td>\n",
       "      <td>2059143248</td>\n",
       "      <td>ihooper</td>\n",
       "      <td>1007478642</td>\n",
       "      <td>[#Ukrainewashed, #WarPreparedness]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00:00:00</td>\n",
       "      <td>Marrying a second or third cousin once removed...</td>\n",
       "      <td>2008209828</td>\n",
       "      <td>wrightnicholas</td>\n",
       "      <td>1039258480</td>\n",
       "      <td>[#FamilyTree, #GeneticFacts]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00:00:00</td>\n",
       "      <td>It's truly disgraceful how the Indian National...</td>\n",
       "      <td>2001239278</td>\n",
       "      <td>michael51</td>\n",
       "      <td>1021455936</td>\n",
       "      <td>[#RationChorCongress]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  timestamp                                               text     text_id  \\\n",
       "0  00:00:00  Running a business means juggling countless ad...  2018569761   \n",
       "1  00:00:00  Liz Truss is walking in the lingering shadow o...  2092717718   \n",
       "2  00:00:00  The UK is bracing for war as government buildi...  2059143248   \n",
       "3  00:00:00  Marrying a second or third cousin once removed...  2008209828   \n",
       "4  00:00:00  It's truly disgraceful how the Indian National...  2001239278   \n",
       "\n",
       "               user     user_id                            hashtags mentions  \n",
       "0     danielwoodard  1077866112      [#HRtech, #businessmanagement]       []  \n",
       "1  nelsonjacqueline  1089670430                         [#politics]       []  \n",
       "2           ihooper  1007478642  [#Ukrainewashed, #WarPreparedness]       []  \n",
       "3    wrightnicholas  1039258480        [#FamilyTree, #GeneticFacts]       []  \n",
       "4         michael51  1021455936               [#RationChorCongress]       []  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_posts['timestamp'] = df_posts['timestamp'].dt.time\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave apostrophes in here for better lemmatization performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some helper methods...\n",
    "\n",
    "def lower(text):\n",
    "  return text.lower() # Return lowercase\n",
    "\n",
    "def remove_links(text):\n",
    "  text = re.sub(r'http\\S+', '', text) # remove http links\n",
    "  text = re.sub(r'bit.ly/\\S+', '', text) # remove bitly links\n",
    "  text = text.strip('[link]') # remove [link]\n",
    "  return text\n",
    "\n",
    "def remove_punctuation_and_numbers(text):\n",
    "  text = re.sub(r\"[^\\w\\s'’]\", '', text)  # remove punctuation but keep apostrophes\n",
    "  text = re.sub(r'\\d+', '', text)  # remove numbers\n",
    "  return text\n",
    "\n",
    "def remove_hashtags(text):\n",
    "  text = re.sub(r'#\\w+', '', text)  # remove hashtags and following text\n",
    "  return text.strip()\n",
    "\n",
    "def remove_emails(text):\n",
    "   text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text)\n",
    "   return text.strip()\n",
    "\n",
    "def remove_mentions(text):\n",
    "   text = re.sub(r'@\\w+', '', text) # remove mentions and following text\n",
    "   return text.strip()\n",
    "\n",
    "def remove_twitter_terms(text):\n",
    "    remove_tw_terms = [\"cc\", \"cx\", \"ct\", \"dm\", \"ht\", \"mt\", \"prt\", \"rt\", \"followback\", \"follow back\", \"fb\", \"retweet\", \"retweets\"]\n",
    "    remove_terms_pattern = re.compile(r'\\b(' + '|'.join(re.escape(term) for term in remove_tw_terms) + r')\\b', re.IGNORECASE)\n",
    "    return remove_terms_pattern.sub('', text)\n",
    "\n",
    "# Function to merge spaced-out letters\n",
    "def merge_letter_spacing(text):\n",
    "    text = re.sub(r'(\\b(?:\\w\\s)+\\w\\b)', lambda m: m.group(0).replace(' ', ''), text)\n",
    "    return text\n",
    "\n",
    "# Convert full-width characters to half-width\n",
    "def normalize_full_width(text):\n",
    "  return ''.join(\n",
    "    chr(ord(char) - 0xFEE0) if 0xFF01 <= ord(char) <= 0xFF5E else char\n",
    "    for char in text)\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    return ''.join(char for char in text if ord(char) < 128)\n",
    "\n",
    "def remove_underscore(text):\n",
    "  text = re.sub(r'([_]+)', '', text)\n",
    "  return text\n",
    "\n",
    "def remove_excessive_whitespace(text):\n",
    "  text = re.sub(r'\\s+', ' ', text).strip()\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>running a business means juggling countless ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>z truss is walking in the lingering shadow of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the uk is bracing for war as government buildi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>marrying a second or third cousin once removed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t's truly disgraceful how the indian national ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  running a business means juggling countless ad...\n",
       "1  z truss is walking in the lingering shadow of ...\n",
       "2  the uk is bracing for war as government buildi...\n",
       "3  marrying a second or third cousin once removed...\n",
       "4  t's truly disgraceful how the indian national ..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply all above functions to our posts\n",
    "df_posts['text'] = df_posts['text'].apply(lower)\n",
    "df_posts['text'] = df_posts['text'].apply(remove_links)\n",
    "df_posts['text'] = df_posts['text'].apply(remove_punctuation_and_numbers)\n",
    "df_posts['text'] = df_posts['text'].apply(remove_hashtags)\n",
    "df_posts['text'] = df_posts['text'].apply(remove_emails)\n",
    "df_posts['text'] = df_posts['text'].apply(remove_mentions)\n",
    "df_posts['text'] = df_posts['text'].apply(remove_twitter_terms)\n",
    "df_posts['text'] = df_posts['text'].apply(merge_letter_spacing)\n",
    "df_posts['text'] = df_posts['text'].apply(normalize_full_width)\n",
    "df_posts['text'] = df_posts['text'].apply(remove_non_ascii)\n",
    "df_posts['text'] = df_posts['text'].apply(remove_underscore)\n",
    "df_posts['text'] = df_posts['text'].apply(remove_excessive_whitespace)\n",
    "\n",
    "# Display head to check the results\n",
    "df_posts[['text']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duplicate rows: 39464\n",
      "Unique duplicate tweets: 16399\n"
     ]
    }
   ],
   "source": [
    "# Total duplicate rows\n",
    "total_duplicate_rows = df_posts['text'].duplicated(keep=False).sum()\n",
    "\n",
    "# Number of unique duplicate tweets\n",
    "unique_duplicate_tweets = (df_posts['text'].value_counts() > 1).sum()\n",
    "\n",
    "print(f\"Total duplicate rows: {total_duplicate_rows}\")\n",
    "print(f\"Unique duplicate tweets: {unique_duplicate_tweets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    16399.000000\n",
      "mean         2.406488\n",
      "std          1.475009\n",
      "min          2.000000\n",
      "25%          2.000000\n",
      "50%          2.000000\n",
      "75%          3.000000\n",
      "max        130.000000\n",
      "Name: count, dtype: float64\n",
      "Number of tweets repeated more than 5 times: 117\n"
     ]
    }
   ],
   "source": [
    "# Get the frequency distribution of tweets\n",
    "frequency_distribution = df_posts['text'].value_counts()\n",
    "\n",
    "# Filter for only duplicates (frequency > 1)\n",
    "duplicate_tweet_frequencies = frequency_distribution[frequency_distribution > 1]\n",
    "\n",
    "# Summary statistics\n",
    "print(duplicate_tweet_frequencies.describe())\n",
    "\n",
    "# How many tweets are repeated more than 5 times?\n",
    "highly_duplicated = (duplicate_tweet_frequencies > 5).sum()\n",
    "print(f\"Number of tweets repeated more than 5 times: {highly_duplicated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicate tweets only if the same person posted the same tweet (spam) but extract frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Group by 'user' and 'text' and calculate the frequency of each combination\n",
    "df_posts['frequency'] = df_posts.groupby(['user', 'text'])['text'].transform('count')\n",
    "\n",
    "# Step 2: Drop duplicates based on 'user' and 'text' (keeping the first occurrence)\n",
    "df_posts = df_posts.drop_duplicates(subset=['user', 'text'], keep='first').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets after filtering: 68544\n",
      "English tweets: 68530\n",
      "Tweets with None language: 14\n",
      "Number of non-English tweets: 693\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained language identification model\n",
    "model = fasttext.load_model('../models/lid.176.bin')\n",
    "\n",
    "# Function to detect language using FastText\n",
    "def detect_language_fasttext(text, threshold=0.8):\n",
    "    # Skip tweets with fewer than 3 words\n",
    "    if len(text.split()) < 3:\n",
    "        return None  # Skip these tweets entirely\n",
    "    try:\n",
    "        predictions = model.predict(text)  # Predict the language\n",
    "        lang_code = predictions[0][0].replace(\"__label__\", \"\")  # Extract language code\n",
    "        return lang_code\n",
    "    except Exception as e:\n",
    "        return None  # Skip on exception\n",
    "\n",
    "# Apply language detection to the 'text' column\n",
    "df_posts['language'] = df_posts['text'].apply(detect_language_fasttext)\n",
    "\n",
    "# Filter non-English tweets (ignoring None values)\n",
    "non_english_tweets = df_posts[(df_posts['language'].notna()) & (df_posts['language'] != 'en')]\n",
    "\n",
    "# Filter to keep English tweets AND tweets with None language\n",
    "df_posts = df_posts[(df_posts['language'] == 'en') | (df_posts['language'].isna())].copy()\n",
    "\n",
    "# Print diagnostic information\n",
    "print(f\"Total tweets after filtering: {len(df_posts)}\")\n",
    "print(f\"English tweets: {len(df_posts[df_posts['language'] == 'en'])}\")\n",
    "print(f\"Tweets with None language: {df_posts['language'].isna().sum()}\")\n",
    "print(f\"Number of non-English tweets: {len(non_english_tweets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the non-English tweets to a CSV file\n",
    "non_english_tweets.to_csv('../output/non_english_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en' None]\n"
     ]
    }
   ],
   "source": [
    "print(df_posts['language'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of appearances of each Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets per non-english language:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "language\n",
       "pt    479\n",
       "es    154\n",
       "id     16\n",
       "it      8\n",
       "de      7\n",
       "nl      6\n",
       "tl      6\n",
       "no      3\n",
       "fr      3\n",
       "tr      2\n",
       "ca      2\n",
       "hu      1\n",
       "sw      1\n",
       "uk      1\n",
       "pl      1\n",
       "sv      1\n",
       "fi      1\n",
       "fy      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_counts = non_english_tweets['language'].value_counts()\n",
    "print(\"Number of tweets per non-english language:\")\n",
    "language_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translate the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets translated: 0\n"
     ]
    }
   ],
   "source": [
    "# Initialize translator\n",
    "translator = Translator()\n",
    "# Define languages to translate\n",
    "languages_to_translate = ['PT', 'ES', 'TL']\n",
    "# Filter tweets in the selected languages\n",
    "translated_tweets = non_english_tweets[non_english_tweets['language'].isin(languages_to_translate)].copy()\n",
    "\n",
    "# Function to translate text\n",
    "def translate_text(text):\n",
    "    try:\n",
    "        result = translator.translate(text, dest='en')\n",
    "        return result.text\n",
    "    except Exception as e:\n",
    "        return text  # Return original text if translation fails\n",
    "\n",
    "# Translate all tweets\n",
    "translated_tweets['text'] = translated_tweets['text'].apply(translate_text)\n",
    "\n",
    "# Save to CSV\n",
    "translated_tweets.to_csv('../output/translated_tweets.csv', index=False)\n",
    "print(f\"Number of tweets translated: {len(translated_tweets)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append translated tweets to english tweets dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final number of tweets: 68544\n"
     ]
    }
   ],
   "source": [
    "df_posts = pd.concat([df_posts, translated_tweets], ignore_index=True)\n",
    "\n",
    "print(f\"Final number of tweets: {len(df_posts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export for Topic Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.to_csv('../output/export_for_topic_classification.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the spaCy model\n",
    "Can be installed via `python -m spacy download en_core_web_sm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load en_core_web_sm for spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use lemmatization since stemming can lead to less accurate results (even non-words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info this takes about 4.5m!!\n",
    "df_posts['text'] = df_posts['text'].apply(\n",
    "        lambda text: ' '.join([token.lemma_ for token in nlp(text)])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create output for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.to_csv('../output/preprocessed_for_SA.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts['text'] = df_posts['text'].fillna('').apply(\n",
    "    lambda text: ' '.join([token.text for token in nlp.make_doc(text) if not token.is_stop])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Apostrophes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts['text'] = df_posts['text'].str.replace(r\"[’']\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check again for empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>text_id</th>\n",
       "      <th>user</th>\n",
       "      <th>user_id</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>frequency</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [timestamp, text, text_id, user, user_id, hashtags, mentions, frequency, language]\n",
       "Index: []"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display rows where 'text' is missing (NaN)\n",
    "missing_text_rows = df_posts[df_posts['text'].isnull()]\n",
    "missing_text_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the output to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.to_csv('../output/preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info this can take over 30m!!\n",
    "# spacy.prefer_gpu()  # Prefers GPU but doesn't crash if unavailable\n",
    "# nlp = spacy.load(\"en_core_web_trf\")\n",
    "# def extract_entities(text):\n",
    "#     \"\"\"\n",
    "#     Extracts named entities from text using SpaCy's NER model.\n",
    "\n",
    "#     Args:\n",
    "#     text (str): The text from which to extract named entities.\n",
    "\n",
    "#     Returns:\n",
    "#     list: A list of tuples where each tuple contains (entity_text, entity_label).\n",
    "#     \"\"\"\n",
    "#     if not text or pd.isna(text):\n",
    "#         return []  # Return an empty list if text is missing\n",
    "\n",
    "#     # Process text with SpaCy\n",
    "#     doc = nlp(text)\n",
    "\n",
    "#     # Extract entity text and labels\n",
    "#     entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "#     return entities\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     \"\"\"\n",
    "#     Preprocesses text by removing URLs and emojis while keeping mentions and hashtags intact.\n",
    "\n",
    "#     Args:\n",
    "#     text (str): The original text.\n",
    "\n",
    "#     Returns:\n",
    "#     str: Preprocessed text.\n",
    "#     \"\"\"\n",
    "#     if not text or pd.isna(text):\n",
    "#         return \"\"  # Return empty string if text is missing\n",
    "\n",
    "#     # Remove URLs\n",
    "#     text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
    "#     # Remove emojis\n",
    "#     text = emoji.replace_emoji(text, replace=\"\")\n",
    "    \n",
    "#     return text.strip()\n",
    "\n",
    "# # Create a preprocessed text column\n",
    "# df_posts['preprocessed_text'] = df_posts['text'].apply(preprocess_text)\n",
    "\n",
    "# # Apply NER extraction on the preprocessed text\n",
    "# df_posts['entities'] = df_posts['preprocessed_text'].apply(extract_entities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
